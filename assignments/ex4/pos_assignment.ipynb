{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6a9550d996390dedc3c988bbc66f536",
     "grade": false,
     "grade_id": "cell-e4091b77c5a80b9e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 4: \n",
    "\n",
    "# Released: 27.02.2020\n",
    "# Deadline: 11.03.2020 at midnight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "714312a20c7a33e28764c5a89c563e5d",
     "grade": false,
     "grade_id": "cell-435a89bd8e2ebc98",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "After completing this assignment, you'll be able to perform sequence tagging tasks such as Part-of-Speech tagging with Hiddent Markov Models. Moreover, you'll learn to analyze the performance of your model.\n",
    "\n",
    "KEYWORDS:\n",
    "\n",
    "* Part-of-Speech (POS) tagging \n",
    "* Hidden Markov Models (HMMs)\n",
    "* Viterbi algorithm\n",
    "* Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d505156968cfdffb26d1da557076110e",
     "grade": false,
     "grade_id": "cell-59e6440452465bc6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Data\n",
    "The GUM corpus (https://corpling.uis.georgetown.edu/gum/) annotated with Universal Dependencies POS tags (https://universaldependencies.org/u/pos/).\n",
    "\n",
    "* */home/contentis/aalto/nlp/coursedata/POS/tags_vocab.txt* - vocabulary of UD tags sorted in alphabetical order\n",
    "* */home/contentis/aalto/nlp/coursedata/POS/train.txt* - corpus for training (4219 sentences, 12181 tokens)\n",
    "* */home/contentis/aalto/nlp/coursedata/POS/words_vocab.txt* - vocabulary of the training corpus sorted in alphabetical order\n",
    "* */home/contentis/aalto/nlp/coursedata/POS/test_words.txt* - unlabelled test corpus (1055 sentences, 5262 tokens)\n",
    "* */home/contentis/aalto/nlp/coursedata/POS/test_tagss.txt* - correct tags for test corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45f1576ae33f614e1238d2d02d22fd90",
     "grade": false,
     "grade_id": "cell-82791e2b8a4d0a2f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**Part-of-speech tagging** (**POS tagging**) is the process of annotating words in an input sequence with their corresponding part-of-speech labels. Word's **part-of-speech** gives us more information about the word itself and about its neighboring words (nouns are preceded by determiners and adjectives). For instance, we can use POS tags as features for Named Entitity recognition task: Proper Nouns like names are usually these entities. Moreover, word's **part-of-speech** provides us with an understanding on how to pronounce this word: cOntent if it is a noun and contEnt if it is an adjective. This helps in such tasks as speech recognition and synthesis.\n",
    "\n",
    "In this assignment we're going to create two algorithms for assigning POS tags. They are both based on statistics collected from a corpus annotated with POS tags by humans. First, we will be assigning words the most frequent tag it has been seen with. This will works as a **baseline** we will be trying to beat. Second, we will create an **HMM** model and compare it to our baseline.\n",
    "\n",
    "The POS tagging algorithm is judged by how **accurate** it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9060cd0b915a64ff0f9184ad786f7871",
     "grade": false,
     "grade_id": "cell-2eaa03fbfd600e2c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1\n",
    "## Read the data\n",
    "\n",
    "In this assignment we are lucky to get already pre-processed text. However, be careful with pre-processing your text for POS tagging: your pre-processing steps should match the pre-processing of the corpus you're collecting statistics from. For example, *POS* tag employed in Penn Treebank Project (https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) is only used for *'s*.\n",
    "\n",
    "### Read vocabularies\n",
    "## 1.1\n",
    "The files for word and tag vocabularies contain each vocabulary member on its own line. Write a function to collect these vocabularies as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd82a94e2b523093fc358939b5fdf040",
     "grade": false,
     "grade_id": "cell-7cac9a2a5bf359fe",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read_vocab(file_name):\n",
    "    \"\"\"\n",
    "    this function takes in a path to a vocabulary file, reads the file,\n",
    "    and returns a vocabulary list\n",
    "    \n",
    "    \n",
    "    INPUT: file_name - a path to a file as a string\n",
    "    OUTPUT: vocab - a list of strings. the elements of the list should have the same order as in the file\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    vocab = open(file_name, 'r').read().split()\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "670666e994ad9b9fc2fa9bfab8c14383",
     "grade": true,
     "grade_id": "cell-b12c70ac2a3de7c2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_vocab_path = \"/home/contentis/aalto/nlp/coursedata/POS/dummy_vocab.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(read_vocab(dummy_vocab_path)), list)\n",
    "# check that it's a list of strings\n",
    "assert_equal(type(read_vocab(dummy_vocab_path)[0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_dummy_vocab = ['this', 'is', 'a', 'dummy', 'voabulary', '!']\n",
    "assert_equal(read_vocab(dummy_vocab_path), correct_dummy_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc865a3010549bd2490c6cb7ee76f2f1",
     "grade": false,
     "grade_id": "cell-73fe73e09cff48f2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Read the training corpus\n",
    "## 1.2\n",
    "The training corpus contains 4219 sentences with words labelled with their correct POS tags by humans. Each sentence is located on its own line, the words are separated from each other by whitespaces. The word is separated from its tag like this: Word_/_TAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3864e4c8fdd692aac0160fb42415a901",
     "grade": false,
     "grade_id": "cell-a8126a1c6db8ff3a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read_train(file_name):\n",
    "    \"\"\"\n",
    "    this function takes in a path to a training corpus file, reads the file,\n",
    "    and returns a list of sentences. each sentence is in turn a list of tuples, \n",
    "    where the first element is a word string and the second element is its tag\n",
    "    \n",
    "    INPUT: file_name - a path to a file as a string\n",
    "    OUTPUT: words_and_tags - a list of lists. [[('word1','tag'),('word2', 'tag')],[('word3','tag')]]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    f = open(file_name, 'r').read()\n",
    "    sentences = f.split('\\n')\n",
    "    words_and_tags = []\n",
    "    for sentence in sentences:\n",
    "        tmp = []\n",
    "        for el in sentence.split():\n",
    "            if len(el.split('/')) != 2:\n",
    "                t = el.split('/')[-1]\n",
    "                w = '/_'\n",
    "            else:\n",
    "                w, t = el.split('/')\n",
    "            tmp.append((w[:-1], t[1:]))\n",
    "        words_and_tags.append(tmp)\n",
    "\n",
    "    return words_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53ea45360f41c2afaddea67effd84773",
     "grade": true,
     "grade_id": "cell-bb5bbeed9756fb81",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_train_path = \"/home/contentis/aalto/nlp/coursedata/POS/dummy_train.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(read_train(dummy_train_path)), list)\n",
    "# check that it's a list of lists\n",
    "assert_equal(type(read_train(dummy_train_path)[0]), list)\n",
    "# check that it's a list of lists of tuples\n",
    "assert_equal(type(read_train(dummy_train_path)[0][0]), tuple)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_dummy_train = [[('word1', 'TAG1'), ('word2', 'TAG2')],\n",
    "                       [('word3', 'TAG3'), ('word4', 'TAG4')]]\n",
    "\n",
    "assert_equal(read_train(dummy_train_path), correct_dummy_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b25b13a80bef2894d76e8c7ca1fd4b65",
     "grade": false,
     "grade_id": "cell-0b10622ba9f943e8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Read the test corpus\n",
    "## 1.3\n",
    "The test corpus is located in two separate files. One contains 1055 unlabelled test sentences. Each sentence is located on its own line. Another file contains cortresponding 1055 sequences of tags. Each tag sequence is located on its own line. The words and tags are separated from each other by whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "791ecf4ce09387c190d6209ea15c4feb",
     "grade": false,
     "grade_id": "cell-8e0395f8c4a35597",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_test(file_name):\n",
    "    \"\"\"\n",
    "    this function takes in a path to a test corpus file, reads the file,\n",
    "    and returns a list of sentences. each sentence is in turn a list of words or a list of tags\n",
    "\n",
    "    INPUT: file_name - a path to a file as a string\n",
    "    OUTPUT: test_sents - a list of lists. [['A','B'],['C']]\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    text = open(file_name, 'r').read()\n",
    "    sentences = text.split('\\n')\n",
    "    test_sents = []\n",
    "    for sentence in sentences:\n",
    "        test_sents.append(sentence.split())\n",
    "\n",
    "    return test_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e6c7466cf825e28c4405a12f01e2d68",
     "grade": true,
     "grade_id": "cell-e9f7ba0d5567131a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-b6a43950e1cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# check that it's a list of lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# check that it's a list of lists of strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0massert_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_test_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "dummy_test_path = \"/home/contentis/aalto/nlp/coursedata/POS/dummy_test.txt\"\n",
    "\n",
    "# check that the output of the function is a list\n",
    "assert_equal(type(read_test(dummy_test_path)), list)\n",
    "# check that it's a list of lists\n",
    "assert_equal(type(read_test(dummy_test_path)[0]), list)\n",
    "# check that it's a list of lists of strings\n",
    "assert_equal(type(read_test(dummy_test_path)[0][0]), str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "correct_dummy_test = [['A', 'B', 'C'], ['D', 'E', 'F', 'G']]\n",
    "\n",
    "assert_equal(read_test(dummy_test_path), correct_dummy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b98bbb82d3fd5605a3474056c80e25d",
     "grade": false,
     "grade_id": "cell-7fd18fccf9c1b3f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### read our data by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a058aca7c8cbae520a34174283ca2188",
     "grade": false,
     "grade_id": "cell-ef97ebcb2ad843f6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tags_vocab = read_vocab(\"/home/contentis/aalto/nlp/coursedata/POS/tags_vocab.txt\")\n",
    "words_vocab = read_vocab(\"/home/contentis/aalto/nlp/coursedata/POS/words_vocab.txt\")\n",
    "\n",
    "words_and_tags = read_train('/home/contentis/aalto/nlp/coursedata/POS/train.txt')\n",
    "\n",
    "test_words = read_test(\"/home/contentis/aalto/nlp/coursedata/POS/test_words.txt\")\n",
    "test_tags = read_test(\"/home/contentis/aalto/nlp/coursedata/POS/test_tags.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "794bd9e4a457b5f13280f0b9e917bc28",
     "grade": false,
     "grade_id": "cell-342c25e00e2169b1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 2\n",
    "## Study the data\n",
    "\n",
    "It is always good to look at closely at your data. In this task we're going to study things like: how many words are ambiguous, what the most popular POS in English is, what POS is most likely to start a sentence. \n",
    "\n",
    "### Collect word to tag statistics\n",
    "## 2.1 \n",
    "Using the statistics from our trainin corpus, create a matrix, where rows are words and columns are tags. The cells of this matrix are the number of times a word was seen with some tag.\n",
    "\n",
    "For example, imagine, that our training corpus looks like this:\n",
    "\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/corpus.png\">\n",
    "\n",
    "Then, our word to tag matrix will be as follows:\n",
    "\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/wt.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c24adedad1d056847377ff23ff622f00",
     "grade": false,
     "grade_id": "cell-ad6d2f7baff45b19",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_word_tag_matrix(training_corpus, vocab_words, vocab_tags):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function takes in a training_corpus, it word and tag vocabularies,\n",
    "    and creates a word-tag matrix\n",
    "    \n",
    "    INPUT: \n",
    "    training_corpus - a list of lists. [[('word1','tag'),('word2', 'tag')],[('word3','tag')]]\n",
    "    vocab_words - a list of words\n",
    "    vocab_tags - a list of UD tag labels\n",
    "    \n",
    "    OUTPUT: wt_matrix - an numpy array containg word to tag statistics [len(vocab_words) X len(vocab_tags)]\n",
    "    \"\"\"\n",
    "    \n",
    "    wt_matrix = np.zeros((len(vocab_words), len(vocab_tags)))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for sentence in training_corpus:\n",
    "        for tuple in sentence:\n",
    "            w, t = tuple\n",
    "            if t in vocab_tags and w in vocab_words:\n",
    "                widx = vocab_words.index(w)\n",
    "                tidx = vocab_tags.index(t)\n",
    "                wt_matrix[widx, tidx] += 1\n",
    "    return wt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86f1637bb2e9ddf57368b2e39c7662c3",
     "grade": true,
     "grade_id": "cell-d7227c042c0c4dfc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_train = [[('word1', 'TAG1'), ('word2', 'TAG2')],\n",
    "               [('word3', 'TAG2'), ('word4', 'TAG2'), ('word2', 'TAG1')]]\n",
    "dummy_word_vocabulary = ['word1','word2','word3','word4']\n",
    "dummy_tag_vocabulary = ['TAG1','TAG2']\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_word_tag_matrix(dummy_train, \n",
    "                                          dummy_word_vocabulary,\n",
    "                                          dummy_tag_vocabulary).shape, (4,2))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_wt_dummy_matrix = np.array([[1., 0.],\n",
    "                                    [1., 1.],\n",
    "                                    [0., 1.],\n",
    "                                    [0., 1.]])\n",
    "                                   \n",
    "assert_array_equal(create_word_tag_matrix(dummy_train, \n",
    "                                          dummy_word_vocabulary,\n",
    "                                          dummy_tag_vocabulary), correct_wt_dummy_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64559b824df4d1cdea3eb8b8b96dce48",
     "grade": false,
     "grade_id": "cell-2c0a2778829299c4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### create the word-to-tag matrix by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa0d4e1183194a76896c212f6c74f709",
     "grade": false,
     "grade_id": "cell-0803a70e321906e9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "wt_matrix = create_word_tag_matrix(words_and_tags, words_vocab, tags_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49b6a151f7a6fda5b3532cc10b1112be",
     "grade": false,
     "grade_id": "cell-310ab36e57c271ee",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "###  words and their possible tags / tags and their possible words\n",
    "## 2.2\n",
    "\n",
    "Looking at the matrix we've created, answer the following question:\n",
    "- What is the most frequent tag?\n",
    "- What tag was given to the least number of different words?\n",
    "- What is the maximum number of different tags one word in the training corpus has?\n",
    "- What is the word with the maximum number of different tags?\n",
    "- How many words are unambiguous (words having only one tag)?\n",
    "- What is the proportion of unambiguous word types in the vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2044a5f7b0e62f18d6eeccc41707f5e5",
     "grade": false,
     "grade_id": "cell-ad2ee637a8c861e6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# most_popular_tag = \"ADJ\"\n",
    "most_popular_tag = tags_vocab[np.argmax(np.sum(wt_matrix, axis=0))]\n",
    "\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# least_open_tag = \"ADJ\"\n",
    "least_open_tag = tags_vocab[np.argmin(np.sum(wt_matrix, axis=0))]\n",
    "\n",
    "# type in the answer as an integer.\n",
    "# For example:\n",
    "# max_n_of_different_tags = 2\n",
    "max_n_of_different_tags = np.max(np.sum(np.array(wt_matrix, dtype='bool'), axis=1))\n",
    "\n",
    "# type in the answer as a string. it should be written exactly as in the word vocabulary.\n",
    "# For example:\n",
    "# most_ambiguos_word = \".\"\n",
    "most_ambiguos_word = words_vocab[np.argmax(np.sum(np.array(wt_matrix, dtype='bool'), axis=1))]\n",
    "\n",
    "# type in the answer as an integer.\n",
    "# For example:\n",
    "# n_of_ambiguous_words = 200\n",
    "n_of_ambiguous_words = np.sum(np.sum(np.array(wt_matrix, dtype='bool'), axis=1) == 1)\n",
    "\n",
    "# type in the answer as a float number from 0 to 1.\n",
    "# For example:\n",
    "# part_of_unambiguous_words = 0.2\n",
    "part_of_unambiguous_words_in_vocab = n_of_ambiguous_words / len(words_vocab)\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a7934f50299ac3cb3e2c7acf5f02200",
     "grade": true,
     "grade_id": "cell-64c2794a48a2c36a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "defec3e2249e9ce11f306cda139adb30",
     "grade": false,
     "grade_id": "cell-003d0b98175c7849",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Collect tag-to-tag transition statistics\n",
    "## 2.3\n",
    "\n",
    "Another thing we can easily do with our data is creating a bi-gram language model for tags! We will represent it as a tag-to-tag transition statistics data. \n",
    "\n",
    "As you remember from our language modelling assignment, we also want information about what tag starts a sentence and what tag ends it, so we will need to modify our tag sequences by appending special start-of-sentence **&lt;s>** and end-of-sentence **&lt;/s>** symbols.\n",
    "\n",
    "Create a tag-to-tag transition matrix to capture this information. The first row of the matrix will correspond to the start symbol, other rows are just tags in their alphabetical order. The columns of the matrix are, again, tags in their their alphabetical order, and the last column is an end of sentence tag. Each cell corresponds to the number of times a column tag was seen after a row tag in our training corpus.\n",
    "\n",
    "For our toy corpus this matrix will look this way:\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/tt.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddb588a932e0c11a813a2e9ebce65509",
     "grade": false,
     "grade_id": "cell-0963a4980b066610",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_tag_to_tag_transition_matrix(training_corpus, vocab_tags):\n",
    "    \"\"\"\n",
    "    this function takes in a training corpus and its tag vocabulary,\n",
    "    and return a tag_to_tag_transition_matrix of size [len(tag_vocabulary)+1 X len(tag_vocabulary)+1]\n",
    "    the first row contains the number of times each tag started a sentence\n",
    "    the last column contains the number of times each tag enedd a sentence\n",
    "    \n",
    "    INPUT: \n",
    "    training_corpus - a list of lists. [[('word1','tag'),('word2', 'tag')],[('word3','tag')]]\n",
    "    vocab_tags - a list of UD tag labels\n",
    "    \n",
    "    OUTPUT: tag_transition_matrix - an numpy array containg tag to tag transition statistics [len(tag_vocabulary)+1 X len(tag_vocabulary)+1]\n",
    "    \"\"\"\n",
    "    tag_transition_matrix = np.zeros((len(vocab_tags)+1,len(vocab_tags)+1))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for sentence in training_corpus:\n",
    "        cur = 0\n",
    "        for el in sentence:\n",
    "            w, t = el\n",
    "            nxt = vocab_tags.index(t)\n",
    "            tag_transition_matrix[cur, nxt] += 1\n",
    "            cur = nxt + 1\n",
    "        tag_transition_matrix[cur, -1] += 1\n",
    "\n",
    "    return tag_transition_matrix.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3813a0382874ee581591cc46a46a70b",
     "grade": true,
     "grade_id": "cell-dc35a6a959191ea0",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_train2 = [[('word1', 'TAG1'), ('word2', 'TAG2')],\n",
    "               [('word3', 'TAG1'), ('word4', 'TAG2'), ('word2', 'TAG1')]]\n",
    "\n",
    "dummy_tag_vocabulary = ['TAG1','TAG2']\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(create_tag_to_tag_transition_matrix(dummy_train2, dummy_tag_vocabulary).shape, (3,3))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_tt_dummy_matrix = np.array([[2, 0, 0],\n",
    "                                    [0, 2, 1],\n",
    "                                    [1, 0, 1]])\n",
    "                                   \n",
    "assert_array_equal(create_tag_to_tag_transition_matrix(dummy_train2,dummy_tag_vocabulary), correct_tt_dummy_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "822d7bf31b8ec9620529ef570eaff042",
     "grade": false,
     "grade_id": "cell-52406232eadfe4f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### create and display the tag-to-tag transition matrix by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0acd39a3c39ac3a7f2326e7c986a6212",
     "grade": false,
     "grade_id": "cell-31f2b496b8cdd8b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tt_matrix = create_tag_to_tag_transition_matrix(words_and_tags, tags_vocab)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data=tt_matrix, index=[\"START\"]+tags_vocab, columns=tags_vocab+[\"END\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ca433d47c2882444fb246b54b37d57a",
     "grade": false,
     "grade_id": "cell-a411d2a2dae081a0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "###  tag to tag transions\n",
    "## 2.4\n",
    "\n",
    "Looking at the tag-to-tag transition matrix we've created, answer the following question:\n",
    "- What is the most popular tag bi-gram?\n",
    "- What tag is most likely to follow the adjective tag?\n",
    "- What tag is most likely to precede  interjection?\n",
    "- What tag is most likely to start a sentence?\n",
    "- What tag is most likely to end a sentence?\n",
    "- What tag can never start a sentence according to our training data?\n",
    "- How many tags can never end a sentence according to our training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60cdffbbe44db85115974867081d8cdc",
     "grade": false,
     "grade_id": "cell-4dfa044a9f4fdaa4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "idx = np.unravel_index(np.argmax(df.values), df.values.shape)\n",
    "# type in the answer as a tuple containing two strings. write tags exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# most_popular_tag_bi_gram = ('ADJ', 'ADJ')\n",
    "most_popular_tag_bi_gram = (df.index[idx[0]], df.columns[idx[1]])\n",
    "\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# tag_after_adj = 'ADJ'\n",
    "tag_after_adj = df.columns[np.argmax(df.loc['ADJ'].values)]\n",
    "\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# tag_before_intj = 'ADJ'\n",
    "tag_before_intj = df.index[np.argmax(df.INTJ.values)]\n",
    "\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# start_tag = 'ADJ'\n",
    "start_tag = df.columns[np.argmax(df.loc['START'].values)]\n",
    "\n",
    "# type in the answer as a string. it should be written exactly as in the tag vocabulary.\n",
    "# For example:\n",
    "# end_tag = 'ADJ'\n",
    "end_tag = df.index[np.argmax(df.END.values)]\n",
    "\n",
    "# type in the answer as an integer number.\n",
    "# For example:\n",
    "# n_of_non_final_tags = 15\n",
    "n_of_non_final_tags = np.sum(np.array(df.END.values, dtype='bool') == 0)\n",
    "\n",
    "\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "033d439bcd1f81b530d693ab82a13cae",
     "grade": true,
     "grade_id": "cell-7d22ada9fa0ff180",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ac0eeb4406713796c2479b8c68067d8",
     "grade": false,
     "grade_id": "cell-deccc5f1e168b2fb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 3\n",
    "## BASELINE\n",
    "### Create a baseline\n",
    "## 3.1\n",
    "\n",
    "To understand if our tagger is any good we will need to compare it to some baseline model. One popular approach is to assign each word a tag that it has been labelled the most with in the trainin data.\n",
    "\n",
    "Create a function, that labels test word sequences with the most frequent tags. If some word has several tags with the same frequency, just select the one that comes first alphabetically. Assign the word unseen in the training corpus with the 'X' tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f18bf2d11d49975812453f2a1f3ce187",
     "grade": false,
     "grade_id": "cell-b7d6237171831faa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def baseline(word_tag_matrix , test_words, vocab_words, vocab_tags):\n",
    "    \"\"\"\n",
    "    this function takes in word to tag matrix, test sentences to label, word and tag vocabularies, \n",
    "    and assigns every word in test sentences the most frequent tag it was seen with\n",
    "    \n",
    "    INPUT: \n",
    "    word_tag_matrix - an numpy array containg word to tag statistics [len(vocab_words) X len(vocab_tags)]\n",
    "    test_sents - a list of lists. [['word1','word2'],['word3']]\n",
    "    vocab_words - a list of words in the training corpus\n",
    "    vocab_tags - a list of UD tag labels\n",
    "    \n",
    "    \n",
    "    OUTPUT: test_tags_predicted - predicted tags. a list of lists. [['tag1','tag2'],['tag3']]\n",
    "    \"\"\"\n",
    "    most_freq_tag = {}\n",
    "    for ind, word in enumerate(vocab_words):\n",
    "        tag_ind = np.argmax(word_tag_matrix[ind])\n",
    "        most_freq_tag[word] = vocab_tags[tag_ind]\n",
    "        \n",
    "    test_tags_predicted = []\n",
    "    # YOUR CODE HERE\n",
    "    for sentence in test_words:\n",
    "        tmp = []\n",
    "        for word in sentence:\n",
    "            if word in most_freq_tag:\n",
    "                tmp.append(most_freq_tag[word])\n",
    "            else:\n",
    "                tmp.append('X')\n",
    "        test_tags_predicted.append(tmp)\n",
    "    return test_tags_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7295f27c400486b3061018df43d1f2e",
     "grade": true,
     "grade_id": "cell-af1a8d45182cf117",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "wt_dummy_train = np.array([[1., 0.],\n",
    "                           [1., 1.],\n",
    "                           [1., 0.],\n",
    "                           [0., 1.]])\n",
    "\n",
    "dummy_word_vocabulary = ['word1','word2','word3','word4']\n",
    "dummy_tag_vocabulary = ['TAG1','TAG2']\n",
    "dummy_test = [['word1','word2','word5']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a list\n",
    "assert_equal(type(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary)),list)\n",
    "# check that the output is a list of lists\n",
    "assert_equal(type(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary)[0]),list)\n",
    "# check that the output is a list of lists of strings\n",
    "assert_equal(type(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary)[0][0]),str)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_dummy_tags = [['TAG1', 'TAG1', 'X']]\n",
    "                                   \n",
    "assert_array_equal(baseline(wt_dummy_train, dummy_test, dummy_word_vocabulary, dummy_tag_vocabulary),\n",
    "                   correct_dummy_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f125182b5121618d2295cee97dd70ac",
     "grade": false,
     "grade_id": "cell-de6d747117eae92c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Evaluate accuracy\n",
    "## 3.2\n",
    "Create a function to estimate how accurate our POS tagging algorithm is. It should produce the percentage of tags that were assigned correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9aecbd3c2995dd2f6f7eafa77276749d",
     "grade": false,
     "grade_id": "cell-df7704b7fb37b913",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_predicted):\n",
    "    \"\"\"\n",
    "    this function takes in true labels for the test data and labels that were output by the algorith,\n",
    "    and then returns the percent of labels that were right.\n",
    "    \n",
    "    INPUT: \n",
    "    y_true - a list of lists with right tags for each sentence in test corpus. [['TAG1','TAG2'],['TAG1']]\n",
    "    y_predicted - a list of lists with predicted tags for each sentence in test corpus. [['TAG1','TAG2'],['TAG1']]\n",
    "    \n",
    "    OUTPUT: \n",
    "    accuracy - percentage of correctly predicted tags\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    accuracy = []\n",
    "    for stgt, spred in zip(y_true, y_predicted):\n",
    "        for tgt, pred in zip(stgt, spred):\n",
    "            accuracy.append(tgt == pred)\n",
    "    accuracy = float(np.mean(accuracy)) * 100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79abcbd490695365b53ff55fc78c1ec0",
     "grade": true,
     "grade_id": "cell-a96e463002d7f2d6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_true_y = [['TAG1', 'TAG1', 'TAG2']]\n",
    "dummy_predicted_y = [['TAG1', 'TAG1', 'X']]\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a float number\n",
    "assert_equal(type(accuracy(dummy_true_y,dummy_predicted_y)),float)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check the function is working as expected\n",
    "assert_almost_equal(accuracy(dummy_true_y,dummy_predicted_y), 66.66, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb6ef26eac69e7838540c0b21f0311ed",
     "grade": false,
     "grade_id": "cell-acefdf7555be300d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### find out how accurate our baseline model by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "25026fdded97a9029e84dd15795092f0",
     "grade": false,
     "grade_id": "cell-c0e6ce3eca1fa934",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tags_predicted = baseline(wt_matrix, test_words, words_vocab, tags_vocab)\n",
    "print(accuracy(tags_predicted, test_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a295a67dfd1c8f268307aac0f8b49ee",
     "grade": false,
     "grade_id": "cell-806c302465c18e6f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The accuracy score of your baseline model should be around 85 percent. But it doesn't tell us much about what's going wrong.\n",
    "\n",
    "We will create a confusion matrix. A confusion matrix tells us how many times each true tag was predicted as itself and as some other tag. The rows of the matrix are correct tabels, the columns are all tags it could have been confused with. The cell tells how many times a true tag was predicted as some column tag.\n",
    "\n",
    "Run the cell below to calculate the confusion matrix for our baseline model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e389c8cf4ade762770bd76296eba3ee1",
     "grade": false,
     "grade_id": "cell-3d49377804262b1b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def create_confusion_matrix(y_true, y_predicted): \n",
    "    \n",
    "    y_true_as_array_of_tags = [tag for sent in y_true for tag in sent]\n",
    "    y_predicted_as_array_of_tags = [tag for sent in y_predicted for tag in sent]\n",
    "    \n",
    "    cm = confusion_matrix(y_true_as_array_of_tags, y_predicted_as_array_of_tags)\n",
    "    \n",
    "    return cm\n",
    "   \n",
    "cm = create_confusion_matrix(tags_predicted, test_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c257c81a7894a423a3f809de9234372d",
     "grade": false,
     "grade_id": "cell-ee6cfe798f33d3d2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Normalized Confusion matrix\n",
    "## 3.3\n",
    "\n",
    "You can already inspect the confusion matrix above, but let's be honest, raw counts are hard to compare and plot. \n",
    "We will need to normalize our matrix: to make a number of predictions of every tag sum to 1. This will help us to compare mistakes made for both frequent and infrequent tags. The true labels in our confusion matrix are marked as rows, the cells in these rows should correspond to the fraction of times this tag was predicted as a tag that marks the column.\n",
    "\n",
    "Create a function, that takes in a matrix and normalizes its values across a given axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce094eea6245e8ef6e6060b30ddbdb8b",
     "grade": false,
     "grade_id": "cell-57be8e4e286e0d0a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix, axis):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function takes in a matrix, sums its values across a given axis (columns if axis = 0, rows if axis = 1), \n",
    "    and normalizes its cell values according to this sum.\n",
    "    \n",
    "    INPUT: \n",
    "    matrix - a 2d numpy array\n",
    "    \n",
    "    OUTPUT: \n",
    "    normalized_matrix - a 2d numpy array\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    a = np.sum(matrix, axis=axis)\n",
    "    if axis:\n",
    "        normalized_matrix = (matrix.T / a).T\n",
    "    else:\n",
    "        normalized_matrix = matrix / a\n",
    "    return normalized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c099cbc56dd37cf1437b72e428b1871",
     "grade": true,
     "grade_id": "cell-9318385c5c32a031",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_array_almost_equal\n",
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_matrix = np.array([[1,2],\n",
    "                         [3,3]])\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check the shape of the matrix\n",
    "assert_equal(normalize_matrix(dummy_matrix, 1).shape, (2,2))\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check that the matrix has the right values in the right places\n",
    "correct_normalised_matrix_axis_zero = np.array([[0.25, 0.4 ],\n",
    "                                                [0.75, 0.6 ]])\n",
    "correct_normalised_matrix_axis_one = np.array([[0.33, 0.67],\n",
    "                                               [0.5 , 0.5 ]])\n",
    "\n",
    "assert_array_almost_equal(normalize_matrix(dummy_matrix, 0), correct_normalised_matrix_axis_zero, 2)\n",
    "assert_array_almost_equal(normalize_matrix(dummy_matrix, 1), correct_normalised_matrix_axis_one, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50ce3bbdc3fa0e52da6d3f3017a564ac",
     "grade": false,
     "grade_id": "cell-a231a19b1a47c8cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### visualize normalized confusion matrix by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64495641dfbf8acdff4a33753dcbb526",
     "grade": false,
     "grade_id": "cell-13752418c5a6213e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, vocab_tags):\n",
    "    \n",
    "    plt.figure(figsize = (11,8))\n",
    "    colors = sns.light_palette((220, 50, 20), input=\"husl\", n_colors=80)\n",
    "    ax = sns.heatmap(np.around(cm, 2),\n",
    "                    annot=True,\n",
    "                    linewidths=.8, \n",
    "                    cmap=colors)\n",
    "    ax.set_ylim(bottom = 17, top=0)\n",
    "    ax.set(xticklabels=vocab_tags)\n",
    "    ax.set(yticklabels=vocab_tags)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90) \n",
    "    plt.ylabel(\"True Tags\")\n",
    "    plt.xlabel(\"Predicted Tags\")\n",
    "    plt.show()\n",
    "    \n",
    "# normalize cm by rows\n",
    "cm_normalized = normalize_matrix(cm, 1)\n",
    "plot_confusion_matrix(cm_normalized, tags_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b872c3d382b2290135fe80afaf41347",
     "grade": false,
     "grade_id": "cell-774600f9bc6d75f0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Study the baseline results\n",
    "## 3.4\n",
    "By looking at the visualization of normalized confusion matrix, briefly answer the following questions in the cell below:\n",
    "- What tags were predicted best? How would you explain it?\n",
    "- Why some tags (nouns, proper nouns, verbs...) were predicted as 'X' that often? Why 'X' was never mistaked with some other classes?\n",
    "- How many unseen words were in the test corpus? Does it affect performance of the baseline algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35bd6ff781c13baad9ee4b116f6e79aa",
     "grade": true,
     "grade_id": "cell-e380c308eb26462f",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5c4df33fac63c24aa6cd796806d9aef",
     "grade": false,
     "grade_id": "cell-92be2872dd47c4f1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 4\n",
    "## HMM POS-tagger\n",
    "An **HMM** is a probabilistic sequence model. In our case, given a sequence of words, it computes a probability distribution over possible sequences of POS tags and chooses the best tag sequence.\n",
    "\n",
    "The hidden part of our HMM are tags, because they are some abstract classes that are not directly observed from text sequences. The observed part of out HMM are words these hidden tags produce. \n",
    "The components of our HMM will be:\n",
    "1. T - a set of $N$ POS tags\n",
    "\n",
    "2. $A$ - a transition probability matrix. Each cell $a_{i,j}$ represents a probability of moving from $tag_i$ to $tag_j$: $P(t_j|t_i)$. We also add the start and end probabilities to this matrix, so it has $N+1$x$N+1$ dimensions. The row $a_1$ represents the probabilities of each tag to start a sentence. The last column of transition probability matrix A contains the probability of every tag to end a sentence.\n",
    "\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/A.png\">\n",
    "3. $B$ - an observation likelihood matrix. Each cell $b_{i,j}$ represents a probability of a $word_i$ being generated out of some $tag_j$: $P(w_i|t_j)$\n",
    "\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/B.png\">\n",
    "\n",
    "### Collect probability matrices A and B\n",
    "## 4.1\n",
    "\n",
    "Turns out, we already have everything for our HMM, we just need to turn the frequencies that we collected previously into maximum likelihood probabilities.\n",
    "\n",
    "You can do it by normalizing out word-tag and tag-tag matrices across the appropriate axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9144ea64580326438960e74a5bb4ea6e",
     "grade": false,
     "grade_id": "cell-93d0a82be4ecb461",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# type in the right axis instead of None value to normalize word to tag matrix\n",
    "# For example:\n",
    "# axis_to_normalize_wt_matrix_by = 0\n",
    "axis_to_normalize_wt_matrix_by = 0\n",
    "# type in the right axis instead of None value to normalize tag to tag matrix\n",
    "# For example:\n",
    "# axis_to_normalize_tt_matrix_by = 0\n",
    "axis_to_normalize_tt_matrix_by = 1\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45261620e25003cf3bd4f75b37eda897",
     "grade": true,
     "grade_id": "cell-3c7842041e0c32c6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n",
    "from nose.tools import assert_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a036ccf84437a309ebef1120db45fff",
     "grade": false,
     "grade_id": "cell-85354c5c58ed7b40",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### create HMM probability matrices by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5aadac32ff937000e75ea7aac7534025",
     "grade": false,
     "grade_id": "cell-0d5d406ec54545a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "A = normalize_matrix(tt_matrix,axis_to_normalize_tt_matrix_by)\n",
    "B = normalize_matrix(wt_matrix,axis_to_normalize_wt_matrix_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4ed2bad3e12c20b13e9d3aeb3f6a3a6",
     "grade": false,
     "grade_id": "cell-fdb384eee973ab42",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### display tag transition matrix A by running the cell below\n",
    "Examine that everything looks as you would expect it to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d88b80dda56c661aaeb49be1d52665a",
     "grade": false,
     "grade_id": "cell-f49b67adc0e603ce",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=A, index=[\"START\"]+tags_vocab, columns=tags_vocab+[\"END\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b946353bb2854a70b1a1eb01dd21ead3",
     "grade": false,
     "grade_id": "cell-18670b0bf05424fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### HMM decoding\n",
    "\n",
    "The aim of an HMM decoding is to choose the tag sequence $t^n_1$ that will be the most probable given the observation sequence of $n$ words $w^n_1$: $\\underset{t^n_1}{\\arg\\max}P(t^n_1|w^n_1)$\n",
    "\n",
    "Using Bayes' rule, we can very conveniently flip this into: $\\underset{t^n_1}{\\arg\\max}\\frac{P(w^n_1|t^n_1)P(t^n_1)}{P(w^n_1)}$. You can also notice, that we don't need denominator for maximizing the tag sequence probability. Thus:\n",
    "\n",
    "*best tag sequence* $= \\underset{t^n_1}{\\arg\\max}P(w^n_1|t^n_1)P(t^n_1)$\n",
    "\n",
    "Now we can simplify it even further by assuming:\n",
    "\n",
    "1. the probability of a particular tag depends only on the previous tag\n",
    "2. the probability of an observed word depends only on the tag that produced this word\n",
    "\n",
    "*best tag sequence* $= \\underset{t^n_1}{\\arg\\max}\\displaystyle\\prod_{i=1}^{n}P(w_i|t_i)P(t_i|t_{i-1})$\n",
    "\n",
    "Lucky us, we've already collected probabilities $P(w_i|t_i)$ in the matrix A, and $P(t_i|t_{i-1})$ in the matrix B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi algorithm\n",
    "\n",
    "The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states. In our case, its output is the most probable sequence of POS tags and its probability for some word sequence.\n",
    "\n",
    "* STEP 1: get a sequence you want to tag:\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/seq.png\">\n",
    "* STEP 2: create a path probability matrix $V$ with the shape (number of tags, number of words to tag). \n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/table1.png\">\n",
    "Each cell $V_{j,i}$ represents the probability that the HMM has tag $j$ after seeing $i$ words and passing through the most probable tag sequence $t_1,t_2...t_{i-1}$.\n",
    "This most probable path of tags so far is represented as maximum over all previous tag sequences. The probabilities of $V_{i,j}$ are computed by starting from the most probable of the extensions of the paths that lead to the current cell.\n",
    "\n",
    "* STEP 3: creat a backpointer table P of shape (number of tags, number of words to tag). You can think of it as a table where to write the most probable tag that generated the word before the word we want to tag now. You can skip the first word, since the tag before it was just the beggining of the sentence. \n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/table2.png\">\n",
    "* STEP 4: start filling the first column of $V$. Each cell $V_{j,1}$ contains a probability of a tag $j$ being the starting tag of the sentence $P(t_j|start)$ multiplied by the probability of the first word in the sequence being generated by this tag $P(word_1|t_j)$\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/table1_1.png\">\n",
    "* STEP 5: move on to fill in the second column of $V$. Each cell $V_{j,2}$ contains a product of two probabilities:\n",
    "    1. the maximum from the products of every value in the previous column of $V$ and every transition probability to the tag $j$. This value keeps balance between what is likely according to the tag language model and what we have seen before.\n",
    "    <img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/max.png\">\n",
    "    2. the probability of the second word in the sequence being generated by the tag $j$ $P(word_2|t_j)$\n",
    "    <img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/max_and_wt.png\">\n",
    "* STEP 6: save the backpointer to the index of the maximum from the products of every value in the previous column of $V$ and every transition probability to the tag $j$ (to the most probable tag of the previous word).\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/backpointer.png\">\n",
    "* STEP 7: compute the rest of the table $V$ as in STEP 5, don't forget to save backpointers as in STEP 6.\n",
    "* STEP 8: compute the final output probabilities of your tag paths by multiplying the last column of V by probability of each tag to end a sentence. \n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/final.png\">\n",
    "* STEP 9: add the last backpointer by chosing the most probable last tag from STEP 8.\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/last_pointer.png\">\n",
    "* STEP 10: trace back the indices in your backpointer table P, starting with one that was output by STEP 9\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/path.png\">\n",
    "* STEP 11: return the most probable tag sequence from the backtraced path.\n",
    "<img src= \"../../../home/contentis/aalto/nlp/coursedata/notebook_illustrations/tags_predicted.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9108385f065f0569c36086de8f424031",
     "grade": false,
     "grade_id": "cell-57a24ce05e92abb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Create Viterbi algorithm\n",
    "## 4.2\n",
    "Write a function for the viterbi algorithm. \n",
    "\n",
    "* Note 1: to avoid numerical underflow, use log probabilities.\n",
    "* Note 2: when you encounter an unseen word, cheat and don't include the observation probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4657295d8c54adff76cda5d26f39b0b4",
     "grade": false,
     "grade_id": "cell-9a4ea49efea89ec4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def viterbi(A, B, word_sequence, tags_vocab, words_vocab):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function takes in HMM matrices A and B, test sequence to tag, word and tag vocabularies,\n",
    "    and returns the tags for the test sequence.\n",
    "    \n",
    "    INPUT: \n",
    "    A - transition probability matrix for POS tags\n",
    "    B - an observation likelihood matrix\n",
    "    word_sequence - a list of word for a test sequence\n",
    "    tags_vocab - a list of UD tag labels\n",
    "    words_vocab - a list of words seen in during training\n",
    "      \n",
    "    OUTPUT: \n",
    "    best_path - a list of tags for the words in a test sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    path_probability_matrix = np.zeros((len(tags_vocab), len(word_sequence)))\n",
    "    backpointer_table = np.full((len(tags_vocab), len(word_sequence)), -1)\n",
    "    \n",
    "    # FILL IN THE FIRST WORD'S COLUMN\n",
    "    # YOUR CODE HERE\n",
    "    if not len(word_sequence):\n",
    "        return []\n",
    "    if word_sequence[0] not in words_vocab:\n",
    "        path_probability_matrix[:, 0] = A[0, :-1]\n",
    "    else:\n",
    "        path_probability_matrix[:, 0] = B[words_vocab.index(word_sequence[0]), :] * A[0, :-1]\n",
    "\n",
    "    for i in range(len(word_sequence) - 1):\n",
    "        tmp = np.multiply(A[1:, :-1].T, path_probability_matrix[:, i]).T\n",
    "        if word_sequence[i+1] not in words_vocab:\n",
    "            path_probability_matrix[:, i + 1] = np.max(tmp, axis=0)\n",
    "        else:\n",
    "            path_probability_matrix[:, i + 1] = np.max(tmp, axis=0)*B[words_vocab.index(word_sequence[i+1]), :]\n",
    "        backpointer_table[:, i] = np.argmax(tmp, axis=0)\n",
    "    p_out = path_probability_matrix[:, -1]*A[1:, -1]\n",
    "    backpointer_table[np.argmax(p_out), -1] = np.argmax(p_out)\n",
    "    best_path = []\n",
    "    for col in path_probability_matrix.T:\n",
    "        best_path.append(tags_vocab[np.argmax(col)])\n",
    "\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ad1ad5f249e076c863b72c04979008b",
     "grade": true,
     "grade_id": "cell-5bb4b8790f00ef97",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "dummy_A = np.array([[3/4,1/4,0],[1/4,3/4,0],[0,0,1.]])\n",
    "dummy_B = np.array([[0,1],[3/4,0],[1/4, 0]])\n",
    "dummy_test = ['red','right','hand']\n",
    "dummy_tags_vocab = [\"ADJ\", \"NOUN\"]\n",
    "dummy_words_vocab = ['hand','red','right']\n",
    "\n",
    "# CHECKING THE GENERAL PROPERTIES OF THE OUTPUT\n",
    "# check that the output is a list\n",
    "assert_equal(type(viterbi(dummy_A, dummy_B, dummy_test, dummy_tags_vocab, dummy_words_vocab)),list)\n",
    "# check that the output is a list of strings\n",
    "assert_equal(type(viterbi(dummy_A, dummy_B, dummy_test, dummy_tags_vocab, dummy_words_vocab)[0]),str)\n",
    "\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING AS IT SHOULD\n",
    "# check the function is giving out the right tag sequence\n",
    "correct_dummy_tags = ['ADJ', 'ADJ', 'NOUN']\n",
    "assert_equal((viterbi(dummy_A, dummy_B, dummy_test, dummy_tags_vocab, dummy_words_vocab)),correct_dummy_tags)\n",
    "\n",
    "# CHECKING THAT THE FUNCTION IS WORKING WITH UNKNOWN WORDS\n",
    "dummy_test2 = ['red','right','leg']\n",
    "correct_dummy_tags2 = ['ADJ', 'ADJ', 'NOUN']\n",
    "assert_equal((viterbi(dummy_A, dummy_B, dummy_test2, dummy_tags_vocab, dummy_words_vocab)),correct_dummy_tags2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb4ff2e5fd77eb860d6401a4fa7f6a5a",
     "grade": false,
     "grade_id": "cell-b2f21c3f133a4264",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluate HMM\n",
    "#### run the cell below to get an accuracy score for your HMM tagger and to plot the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "460ae40ee0690096dc17e5b6da0bc7af",
     "grade": false,
     "grade_id": "cell-329710d59cc3bbad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "hmm_tags_predicted = []\n",
    "for i in range(len(test_words)):\n",
    "    hmm_tags_predicted.append(viterbi(A, B, test_words[i], tags_vocab, words_vocab))\n",
    "\n",
    "    \n",
    "print(accuracy(hmm_tags_predicted, test_tags))\n",
    "\n",
    "cm_hmm = create_confusion_matrix(hmm_tags_predicted, test_tags)\n",
    "cm_hmm_normalized = normalize_matrix(cm_hmm, 1)\n",
    "plot_confusion_matrix(cm_hmm_normalized, tags_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c075c1791aafa842fad23d9f3cb407c1",
     "grade": false,
     "grade_id": "cell-4b4885aa04a6f6dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Compare HMM and Baseline\n",
    "## 4.2\n",
    "Briefly decribe in the cell below the differences in the performance of our HMM and the baseline model. What can be done to further improve the HMM model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b66777b6c5eec5d46e60715cae20f597",
     "grade": true,
     "grade_id": "cell-340e6551f6785f11",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}