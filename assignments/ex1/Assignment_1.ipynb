{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bdfea3fe3ad8d11946e5f933c3bc9d77",
     "grade": false,
     "grade_id": "cell-3f2c101d987217b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 1: Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb152901e16dd97dbd82af38eb32b7c1",
     "grade": false,
     "grade_id": "cell-7511946386ce6d75",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "By completing this assignment you will learn how to handle raw text data. We will explore the frequency distribution of different language units and, then, discuss what this knowledge might give. \n",
    "\n",
    "KEYWORDS:\n",
    "\n",
    "- Frequency distribution\n",
    "- Tokenization\n",
    "- Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "49f0adc39c21672c600a3f554fdc8c2a",
     "grade": false,
     "grade_id": "cell-d05931ac90341fb3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Data\n",
    "\"The Gold-Bug\" by Edgar Allan Poe. \n",
    "### Libraries\n",
    "In this task you'll use:\n",
    "- [NLTK](http://www.nltk.org) â€” a platform to work with human language data.\n",
    "\n",
    "You are also allowed to use other libraries of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa21f2eb44873e37a68bd3447a943a53",
     "grade": false,
     "grade_id": "cell-750a6466a51a9aeb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1\n",
    "## Warm up: The Gold-Bug cipher\n",
    "The data used in this assignment is actually a story about the importance of letter frequencies. The narrator in the story was able to decipher a message leading to a hidden treasure by applying frequency analysis. The cipher used in the story is a substitute cipher where each letter is replaced by a different letter or number.\n",
    "\n",
    "Knowing the frequency of letters in a language is important not only for solving ciphers, but it also has practical applications like data compression. For example, Morse code uses the shortest symbols for the most frequent letters. \n",
    "\n",
    "In this warm-up task you'll need to discover for yourself the frequency distribution of single letters and of letter pairs in English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ca2c9ae0cdf143a267250662de083eb",
     "grade": false,
     "grade_id": "cell-d5d46f8ec022767a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Frequency distribution of letters\n",
    "## 1.1\n",
    "First of all, you need to load the text into the Jupyter Notebook. Create a function that reads the data located in *'/coursedata/the_gold-bug.txt'* file into a string.\n",
    "\n",
    "HINTS: you can employ Python's function open() and read() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b0dcc6f16dc2f9f00cc37dc8393f597",
     "grade": false,
     "grade_id": "cell-6027b855ab1ece6c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def read(file_name):\n",
    "    \"\"\"\n",
    "    this function reads a .txt file into a string\n",
    "    \n",
    "    INPUT:\n",
    "    file_name - a path to the text file\n",
    "    OUTPUT\n",
    "    raw_string - text file as one string\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raw_string = open(file_name, \"r\").read()\n",
    "    \n",
    "    return raw_string\n",
    "\n",
    "raw_gold_bug = read('/coursedata/the_gold-bug.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36c5222c71bab45bc365a155ba039ec0",
     "grade": true,
     "grade_id": "cell-a6ece3ca5d559fd1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(type(read('/coursedata/the_gold-bug.txt')), str)\n",
    "assert_equal(len(read('/coursedata/the_gold-bug.txt')), 76483)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e72e44a26e31046c0e6260d114f4ba93",
     "grade": false,
     "grade_id": "cell-b4bc821c79254eb0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.2\n",
    "To count the letters, clean the text to leave only lowercase alphabetic characters in it.\n",
    "\n",
    "HINT: you may use string methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "251bb23311017de47dd6b00d064010b0",
     "grade": false,
     "grade_id": "cell-b9d5d7c7d769f563",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_letters(raw_string):\n",
    "    \"\"\"\n",
    "    this function takes a raw text string and converts it into an array of lowercase letters\n",
    "    \n",
    "    INPUT:\n",
    "    raw_string - text file in a string format\n",
    "    OUTPUT:\n",
    "    letters - text as a list of only letters\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    letters = []\n",
    "    raw_string = raw_string.lower()\n",
    "    for l in raw_string:\n",
    "        if not l.isalpha():\n",
    "            continue\n",
    "        letters.append(l)\n",
    "    return letters\n",
    "\n",
    "bug_letters = convert_to_letters(raw_gold_bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2a2e512cbd0be9295f21ed5c14411ff",
     "grade": true,
     "grade_id": "cell-372fa8963c1592ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(convert_to_letters(\"Lalala has 3 las.\"), [\"l\",\"a\",\"l\",\"a\",\"l\",\"a\",\"h\",\"a\",\"s\",\"l\",\"a\",\"s\"])\n",
    "assert_equal(type(bug_letters), list)\n",
    "assert_equal(len(bug_letters), 58269)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "91fda450560c15af5f0f00482727a49f",
     "grade": false,
     "grade_id": "cell-0dc5934ca1a0e8dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.3\n",
    "Count how many times each letter occurred in the story. Then, sort the letters so that the most frequent letter would appear first. Put the sorted alphabet in a string.\n",
    "\n",
    "\n",
    "HINT: you may use NLTK or Collections libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc6cdacbad2045466f237bcc057b3ce",
     "grade": false,
     "grade_id": "cell-a3e7f79d12bc50df",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "# HINT: Try nltk.FreqDist or collections.Counter\n",
    "\n",
    "def sort_letters(letters):\n",
    "    \"\"\"\n",
    "    this function takes a text represented as a list of lowercase letters\n",
    "    and outputs: \n",
    "    1. a frequency dictionary of letters\n",
    "    2. a string of letters sorted by their frequencies\n",
    "    \n",
    "    INPUT:\n",
    "    letters - text as a list of only lowercase letters \n",
    "    OUTPUT:\n",
    "    letter_dict - a frequency dictionary of letters\n",
    "    sorted_letters - a string with the alphabet sorted by frequency\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    letter_dict = dict(nltk.FreqDist(letters))\n",
    "    sorted_letters = [l for _, l in reversed(sorted(zip(letter_dict.values(), letter_dict.keys())))]\n",
    "    sorted_letters = \"\".join(sorted_letters)\n",
    "    return sorted_letters, letter_dict\n",
    "\n",
    "bug_letters_sorted, letter_dict = sort_letters(bug_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c955c195a4c8060cb67738ff62bc4b65",
     "grade": true,
     "grade_id": "cell-9b9418b170d2bdae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(sort_letters(\"aaabbc\"), (\"abc\", {\"a\":3,\"b\":2,\"c\":1}))\n",
    "assert_equal(bug_letters_sorted[0], 'e')\n",
    "assert_equal(len(bug_letters_sorted), 26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b26b8e4c1ee7fbdeb35fcb24392ce33f",
     "grade": false,
     "grade_id": "cell-247e03c8c52f0e3e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.4\n",
    "The frequencies of letters differ drastically. That means the probabilities of seeing each letter are also different.\n",
    "Look at your frequency dictionary: \n",
    "* what is the probability to see the most frequent letter? \n",
    "* what is the probability to see the least frequent one?\n",
    "\n",
    "Type your answer in the cell below. You can create an additional cell to do calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7c8bd0ea03912abef1986668a1fc979",
     "grade": false,
     "grade_id": "cell-49154afe71312e96",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def mle_probability_of_letter(letter_dict, letter):\n",
    "    \"\"\"\n",
    "    Computes the maximum likelihood estimate (ie. relative frequency in corpus)\n",
    "    of the given letter based on the given corpus statistics\n",
    "    INPUT:\n",
    "    letter_dict - a frequency dictionary of letters\n",
    "    letter - which letter to compute probability for\n",
    "    OUTPUT:\n",
    "    probability - probability of the given letter\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    probability = letter_dict[letter]/sum(letter_dict.values())\n",
    "    return probability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d1d2c15ae4bff48f4ef71838c23d37a",
     "grade": true,
     "grade_id": "cell-5ed0a325980e2590",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "assert_almost_equal(mle_probability_of_letter({\"a\":5, \"b\":5}, \"a\"), 0.5)\n",
    "assert_almost_equal(mle_probability_of_letter({\"a\":5, \"b\":5, \"c\":0, \"d\":10}, \"a\"), 0.25)\n",
    "\n",
    "p_mfl = mle_probability_of_letter(letter_dict, bug_letters_sorted[0])\n",
    "assert_almost_equal(p_mfl, 0.13125332509567694, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3089ec220ec595bf864cb961943823f8",
     "grade": false,
     "grade_id": "cell-ff1e21f485bb65ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.5 \n",
    "### Frequency distribution of letter combinations\n",
    "Some combinations of language units are more likely than other. You'll see it in a minute.\n",
    "\n",
    "There are 26 letters in English alphabet. How many possible two-letter combinations are there according to combinatorics? For example, if we have an alphabet of 3 letters **a**, **b** and **c**. We can have 9 combinations: **aa**, **bb**, **cc**, **ab**, **ac**, **ba**, **bc**, **ca**, **cb**.\n",
    "\n",
    "Type your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8708e8583c93a3cfeda32ae3afb0d91b",
     "grade": false,
     "grade_id": "cell-e8239fe099ac2509",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def number_of_combinations(number_of_letters, sequence_len):\n",
    "    \"\"\"\n",
    "    this function takes a number of letters in an alphabet and the desired length of letter combinations\n",
    "    and outputs the numberof all possible letter combinations of length \"sequence_len\". \n",
    "    The combination can contain the same letter \"sequence_len\" times.\n",
    "    \n",
    "    INPUT:\n",
    "    number_of_letters - a number of letters in an alphabet\n",
    "    sequence_len - a length of the combination of letters\n",
    "    OUTPUT:\n",
    "    num_of_combinations - the numberof all possible letter combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    num_of_combinations = number_of_letters**sequence_len\n",
    "    return num_of_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da6885e0eca8ccd7e81de83d939c0743",
     "grade": true,
     "grade_id": "cell-305f8cc935f8beab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(number_of_combinations(2,2), 4)\n",
    "assert_equal(number_of_combinations(2,1), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229eac894c33bd142a9a0fc38357573f",
     "grade": false,
     "grade_id": "cell-b8c1ea1279ede03e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.6\n",
    "\n",
    "Same as with single letters, some sequences of language units are more probable than the other. Not all combinations of two letters are possible in English. This fact can be used in such applications as predictive texting: your phone suggests what might be the next word you need.\n",
    "\n",
    "In the following exercise, you'll need to count all combinations of two letters that appeared in the text. For this, you'll create a new function. It uses a sliding window of two letters, records what letters it sees at every step and then counts how many times it encountered each two-letter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97c238cec8260aade8b04d68f757b7df",
     "grade": false,
     "grade_id": "cell-faf866b8fc92dc87",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_pairs(letters):\n",
    "    \"\"\"\n",
    "    this function takes a text represented as a list of lowercase letters\n",
    "    and converts it into a sorted list of tuples, where the first element\n",
    "    is a two-letter string, and the second element is the frequency of this letter pair.\n",
    "    the first element of a list should be a tuple for the most frequent pair.\n",
    "    \n",
    "    INPUT:\n",
    "    letters - text as a list of only lowercase letters \n",
    "    OUTPUT:\n",
    "    pairs_sorted - a list of tuples (pair, frequency) sorted by the frequency element\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    pairs = {}\n",
    "    for i in range(len(letters) - 1):\n",
    "        key = letters[i]+letters[i + 1]\n",
    "        if not key in pairs.keys():\n",
    "            pairs[key] = 1\n",
    "        else:\n",
    "            pairs[key] += 1\n",
    "\n",
    "    pairs_sorted = [(l, f) for f, l in reversed(sorted(zip(pairs.values(), pairs.keys())))]\n",
    "    return pairs_sorted\n",
    "\n",
    "bug_pairs_sorted = count_pairs(bug_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48936eb05b83fef20ba83861c567a426",
     "grade": true,
     "grade_id": "cell-a02209796380d0cc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "assert_equal(type(bug_pairs_sorted), list)\n",
    "assert_equal(type(bug_pairs_sorted[0]), tuple)\n",
    "assert_equal(bug_pairs_sorted[0][1], 1800)\n",
    "assert_equal(count_pairs(\"aaaabbabc\")[:2], [(\"aa\",3),(\"ab\",2)])\n",
    "assert(count_pairs(\"aaaabbabc\")[2] in [(\"ba\",1), (\"bb\", 1), (\"bc\", 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd035f58f05cafc5f8fa331d3ef18969",
     "grade": false,
     "grade_id": "cell-dcb81e3883d219e6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1.7\n",
    "Using the frequency list you created, answer the following questions in the cell below:\n",
    "\n",
    "1. How many different two-letter combinations have you encountered in our data?\n",
    "2. What is the most frequent two-letter combination in English?\n",
    "3. What is the probability of seeing a pair where both letters are the same?\n",
    "\n",
    "You can create an additional cell for calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c51f21c878074f6227d9ef26f8ffadf5",
     "grade": false,
     "grade_id": "cell-f59c76bc63d556f5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# write the number of different two-letter combinations\n",
    "n_pairs_data = len(bug_pairs_sorted) ##FILL IN THE ANSWER\n",
    "# write the most frequent two-letter combination\n",
    "most_frequent_pair = bug_pairs_sorted[0][0] ##FILL IN THE ANSWER\n",
    "# write the maximum likelihood estimate of the probability of seeing a pair where both letters are the same\n",
    "p_same_letters = None ##FILL IN THE ANSWER\n",
    "\n",
    "# YOUR CODE HERE\n",
    "same = 0\n",
    "not_same = 0\n",
    "for s in bug_pairs_sorted:\n",
    "    if s[0][0] == s[0][1]:\n",
    "        same += s[1]\n",
    "    else:\n",
    "        not_same += s[1]\n",
    "p_same_letters = same / (same + not_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519\n",
      "th\n",
      "0.03459875060067275\n"
     ]
    }
   ],
   "source": [
    "print(len(bug_pairs_sorted))\n",
    "print(bug_pairs_sorted[0][0])\n",
    "print(p_same_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb77988266432ef0ac5dcfed2c560579",
     "grade": true,
     "grade_id": "cell-30230a0028dd5352",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e20c026020253e088609b767c7b0cad8",
     "grade": false,
     "grade_id": "cell-9715ee2e39d6f222",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 2\n",
    "## Word Tokenizer\n",
    "In this task, you will create a function that splits the text into more elaborate units than just letters: words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b8b62a5fef482bbddb0669a8b8dd289",
     "grade": false,
     "grade_id": "cell-3bfa916e5c65d448",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Text data is a part of virtually any NLP application. Sometimes you're lucky, and instead of plain raw text you get nice and clean one, but this is not always the case. Before getting your hands dirty with your actual application, you would most probably need to perform some manipulation to the text. For instance, separate it into words and sentences, remove unwanted symbols. Different tasks require different preprocessing techniques. In this task we'll use some simple ones.\n",
    "\n",
    "It's not trivial to separate words from a string of text. The first thing that needs to be decided is what to count as a word. Should punctuation and numbers be considered words? Should *frogs* and *frog* be considered the same word? What about *Frog*, *frog* and *FROG*? Before answering those questions, let's make sure we are on the same page and discuss some terminology.\n",
    "\n",
    "When talking about words, we can mean several different things: lemmas, word types and word tokens.\n",
    "\n",
    "* **Lemma** - an identifier of a set of lexical forms sharing the same stem (*run* is the lemma for *runs* and *running*), a dictionary form of a word.\n",
    "* **Word type** - a distinct unit in a text (all the instances of *runs* are counted once).\n",
    "* **Word token** - every instance of word occurrence (every instance of *runs* counted as a separate word token).\n",
    "\n",
    "Thus:\n",
    "* **Tokenization** - a process of separating out word tokens from text\n",
    "* **Lemmatization** - a process of assigning a group of word forms their lemma, and further separating out these lemmas from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f97a2c576840fe25ebd58215631ddd9",
     "grade": false,
     "grade_id": "cell-8d708d0714a80d37",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Generally, English doesn't require lemmatization since it has quite a limited number of word forms. For this reason, we'll leave this task out, for now, and focus on tokenization instead.\n",
    "\n",
    "Let's create a tokenizer that considers numbers and punctuation as tokens and doesn't separate hyphenated words like *dum-dum*. For that you'll need:\n",
    "- regular expressions\n",
    "- string operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "151575762409f8358802cda825e0b28f",
     "grade": false,
     "grade_id": "cell-91014079a96c570a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.1\n",
    "Let's start off by separating words just by whitespaces and see what happens to our dummy sentence example: *It's a dum-dum example, we'll place it here to prove a point. Also look at this number: 300.99.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72f8f3e6cf29ff631480ac5362744852",
     "grade": false,
     "grade_id": "cell-323fdf945301cde7",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'dum-dum', 'example,', \"we'll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point.', 'Also', 'look', 'at', 'this', 'number:', '300.99.']\n"
     ]
    }
   ],
   "source": [
    "dumb_example = \"It's a dum-dum example, we'll place it here to prove a point. Also look at this number: 300.99.\"\n",
    "\n",
    "def tokenize(raw_string):    \n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT:\n",
    "    raw_string - text file in a string format\n",
    "    OUTPUT:\n",
    "    space_tokenized - list of strings\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    space_tokenized = raw_string.split()\n",
    "    \n",
    "    return space_tokenized\n",
    "\n",
    "dum_dum_example = tokenize(dumb_example)\n",
    "print(dum_dum_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2eb9b4c23076866bb60168b01d8a0603",
     "grade": true,
     "grade_id": "cell-f63d5a4956e3323c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(dum_dum_example[0], \"It's\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0dec62e21be1fccc3dd48ca21553376d",
     "grade": false,
     "grade_id": "cell-ad4e2d19588ca13a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As can be seen from the dummy example, it's not enough to just separate the words by the whitespaces. We get tokens like *'example,'*, *'point.'* and *'number:'*. Instead, we would actually like to have punctuation marks as separate tokens, but keep them inside the items like prices and numbers (4.99). Thus, we need something more clever.\n",
    "## 2.2\n",
    "For these means, you'll need to write a regular expression. It should:\n",
    "- match all alphanumeric strings with hyphen, apostrophe or point inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fb421b573fc046a223770c6a8409a72",
     "grade": false,
     "grade_id": "cell-3c1fce0956ef8f25",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'dum-dum', \"we'll\", '300.99']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "words_with_inside = \"\\w+['-.]\\w+\"\n",
    "\n",
    "print(re.findall(words_with_inside, dumb_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d48943bd987cc2baa4bd9f78d4eb3e52",
     "grade": true,
     "grade_id": "cell-7936f42e9fa94a1e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(re.findall(words_with_inside, \"It's a dum-dum example, we'll place it here to prove a point. Also look at this number: 300.99.\"), \n",
    "             [\"It's\", \n",
    "               'dum-dum', \n",
    "               \"we'll\", \n",
    "               '300.99'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "857715174a9a8d2819b427ea8ccd41a0",
     "grade": false,
     "grade_id": "cell-2ca1b072933f9863",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2.3\n",
    "Now, let's add a disjunction to our regex:\n",
    "- match either all words with hyphen, apostrophe or point inside OR any non-whitespace character followed by any word character between zero and unlimited times.\n",
    "\n",
    "HINT: google what a *word character* is in a regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "979c6e2b12ad167216b824dd1f2c62da",
     "grade": false,
     "grade_id": "cell-7b7959675b9667af",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's\", 'a', 'dum-dum', 'example', ',', \"we'll\", 'place', 'it', 'here', 'to', 'prove', 'a', 'point', '.', 'Also', 'look', 'at', 'this', 'number', ':', '300.99', '.']\n"
     ]
    }
   ],
   "source": [
    "words_with_inside_and_stuff = \"(\\w+['-.]\\w+|\\S\\w*)\"\n",
    "\n",
    "\n",
    "print(re.findall(words_with_inside_and_stuff, dumb_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "967db111951c191dd0de26659201b260",
     "grade": true,
     "grade_id": "cell-d41a29579efb3e0b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(re.findall(words_with_inside_and_stuff, dumb_example)[-1], '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7f58fe7c010daa63f9a7f3f439181f1",
     "grade": false,
     "grade_id": "cell-0868a2120ad8852f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you've already noticed, the process of creating a tokenizer is pretty complicated. There are many more things to be considered, and a tokenizer should be chosen in accordance with a task. For example, we might also want to capture abbreviations (U.S.A.), percentages (82%) or URLs.\n",
    "\n",
    "Luckily, there are already several good tokenizers implemented for. For instance, the NLTK package has several. \n",
    "\n",
    "## 2.4\n",
    "Let's tokenise our text using the Treebank tokenizer. It uses regular expressions to tokenize text as in Penn Treebank. Don't forget to lowercase the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f981df6d6ea18db51263b11bfb487dd0",
     "grade": false,
     "grade_id": "cell-b1ec48f823b7c5c4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "def tokenize_and_lowercase(raw_string):\n",
    "    \"\"\"\n",
    "    this function takes a raw text string and converts it into an array of lowercased word\n",
    "    tokens using Penn Treebank tokenizer.\n",
    "    \n",
    "    INPUT:\n",
    "    raw_string - text file in a string format\n",
    "    OUTPUT:\n",
    "    tokens - text as a list of lowercased word tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokens = raw_string.lower()\n",
    "    tokens = TreebankWordTokenizer().tokenize(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokenized_bug = tokenize_and_lowercase(raw_gold_bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "060a2826ff56bd58c6a22eb6918b7344",
     "grade": true,
     "grade_id": "cell-277001137d76c0ba",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(tokenized_bug), 16290)\n",
    "assert_equal(tokenized_bug[0], 'the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36e09dddb13cea4de0370587ca05c1ab",
     "grade": false,
     "grade_id": "cell-c5b82c31e7e96a54",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 3\n",
    "## Word frequencies\n",
    "In this task you will explore the distribution of word frequencies.\n",
    "## 3.1\n",
    "Let's count how many times each word token occurred in the story. Write a function that returns a frequency dictionary.\n",
    "\n",
    "Explore the data. What are some of the most common tokens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb26e2cff6aaa58bc42b5dfe3f31e06b",
     "grade": false,
     "grade_id": "cell-a8f655518f58b3fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#HINT: simply use nltk.FreqDist or collections.Counter again.\n",
    "\n",
    "def count_tokens(tokenized_text):\n",
    "    \"\"\"\n",
    "    this function takes a list of tokens and converts it into a frequency dictionary\n",
    "    \n",
    "    INPUT:\n",
    "    tokenized_text - text as a list of tokens \n",
    "    OUTPUT:\n",
    "    freq_dict - frequency dictionary of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    freq_dict = dict(nltk.FreqDist(tokenized_text))\n",
    "    \n",
    "    return freq_dict\n",
    "\n",
    "freq_dict = count_tokens(tokenized_bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7946b3d304e997eadaf7326c25848fec",
     "grade": true,
     "grade_id": "cell-727ab27821e90861",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(freq_dict[','], 1302)\n",
    "assert_equal(len(freq_dict) , 3071)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e923f082400af4ba95486730be65e6a9",
     "grade": false,
     "grade_id": "cell-fb0a4f7abc9b2f63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3.2\n",
    "As you can see, the most frequent words are not specific to the Poe's story, but are pretty much the same across English language.  \n",
    "\n",
    "In information theory, the more likely an event to occur, the less information it contains. Thus, if an event is not a surprise, it's simply \"old news\". For some natural language applications, it means that words like *to* and *the* don't tell anything important about a text. They are not helpful in recognising its topic or its author.\n",
    "\n",
    "\n",
    "Such frequent uninformative words are called **stop words**, and, in some cases, they can simply be cleaned out from data. There exist prepared lists of such words in English. Let's remove the ones provided by NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91e1c11f2b4d7bd6d8edfa464e4cc4dc",
     "grade": false,
     "grade_id": "cell-fb4b2ec2f039e056",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/spindll1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words_english = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(tokenized_text, stop_words):\n",
    "    \"\"\"\n",
    "    this function takes a list of tokens and removes stop words from it\n",
    "    \n",
    "    INPUT:\n",
    "    tokenized_text - text as a list of tokens \n",
    "    stop_words - a list of words to remove\n",
    "    OUTPUT:\n",
    "    clean_text - text as a list of tokens with stop words removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    clean_text = tokenized_text.copy()\n",
    "    i = 0\n",
    "    for w in stop_words:\n",
    "        while w in clean_text:\n",
    "            clean_text.remove(w)\n",
    "\n",
    "    return clean_text\n",
    "    \n",
    "clean_bug = remove_stop_words(tokenized_bug, stop_words_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45dc7c516a18f56298354ad683aca496",
     "grade": true,
     "grade_id": "cell-09346ef37946973d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(len(clean_bug), 9211)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "Great! Now you have a clean test that can further be used for such tasks as \n",
    "\n",
    "Last thing. What was the percentage of stop words in the tokenized text?\n",
    "Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7504f5a2bff3788320dcba187807dc74",
     "grade": false,
     "grade_id": "cell-9d0bf9d7bb8f3c8d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# fraction of stop words in the tokenized text (number between 0 and 1)\n",
    "fraction_of_stop_words = 0.565438919582566\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67254ade548dcf6f1ca203bb33b9d600",
     "grade": true,
     "grade_id": "cell-6bd0960480895cd5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565438919582566\n"
     ]
    }
   ],
   "source": [
    "print(len(clean_bug)/len(tokenized_bug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
