{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f12428dc06a9630f22a5cccd65928b76",
     "grade": false,
     "grade_id": "cell-06d639cce7633bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "ELEC-E5550 - Statistical Natural Language Processing\n",
    "# SET 3: N-gram language models\n",
    "\n",
    "# Released: 22.1.2019\n",
    "# Deadline: 5.2.2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f03d4ad359534798b03bc53cb0dc486",
     "grade": false,
     "grade_id": "cell-3bfa793aa2cd6b17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After completing this assignment, you'll understand how statical language models can be estimated. You'll be able to evaluate them and to generate text using them.\n",
    "\n",
    "KEYWORDS:\n",
    "\n",
    "- Language models\n",
    "- N-grams\n",
    "- Perplexity\n",
    "- Additive smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ae5245747ed32f1e9ef72597a7a4775",
     "grade": false,
     "grade_id": "cell-eb34778ec1241a7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Data\n",
    "Jane Austen's Pride and Prejudice, in `/coursedata/pride-and-prejudice.txt`\n",
    "\n",
    "### Libraries\n",
    "In this task you'll use the following libraries:\n",
    "- [random](https://docs.python.org/3/library/random.html) -  is a module that implements pseudo-random number generators for various distributions.\n",
    "- [maths](https://docs.python.org/3/library/math.html) - is a module providing access to the mathematical functions defined by the C standard.\n",
    "- [NLTK](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "167924bc3c21e6dcddd923983809e3b7",
     "grade": false,
     "grade_id": "cell-ecfa9a14c6632972",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## TASK 1\n",
    "## Warm up: estimate probability scores by hand\n",
    "As you've already noticed in the first assignment, different language sequences are not equally likely to occur. We've only looked at word frequencies and at frequencies of letter sequences, but what if we want to estimate how probable it is to see some sentence? Well, for this purpose you'll need a **language model**.\n",
    "\n",
    "A **language model** predicts the following word (or other symbol) given the observed history.\n",
    "$P(w_i| w_{i−1} . . . w_0)$.\n",
    "**Language models** are useful, for example, in the task of speech recognition. They help to distinguish between homophones (words that sound the same but have different meanings), for example, _\"to\"_ and _\"too\"_ in _\"I love you too\"_ and _\"I love you to death\"._\n",
    "\n",
    "### What is an n-gram language model?\n",
    "An **n-gram language model** approximates the probability of a word given all the previous words by using only the conditional probability of the $N-1$ preceding words. This approach is based on the Markov assumption: the next word depends only on a fixed-size window of previous words and not on the whole history: $P(w_i| w_{i−1} . . . w_{i-h})$\n",
    "<img src= \"../../../coursedata/notebook_illustrations/n_gram.png\">\n",
    "To use an n-gram language model, we need estimates of the probability of seeing a particular word given the recent\n",
    "history. For instance, in the 3-gram model, the history is 2 words.\n",
    "\n",
    "* 3-gram: (say hello **to**) \n",
    "* 2 words of history\n",
    "* 1 word for **prediction**\n",
    "\n",
    "\n",
    "An intuitive way to estimate probabilities is calculating **maximum likelihood estimation** (MLE). To get the MLE estimate of an n-gram model we get counts from a corpus and normalize these counts to lie between 0 and 1. In the case of bigram language model, when we want to get a probability of some particular bigram $P(w_n|w_{n−1})$, we’ll compute the count of the bigram $C(w_{n−1}w_n)$ and normalize it by the sum of all the bigrams that start with the same first word $w_{n−1}$. It is easy to notice that the sum of all bigram counts that have the same fist word is simply the unigram count for that word $w_{n−1}$. Thus, we get:\n",
    "\n",
    "$P(w_n|w_{n−1}) = \\frac{C(w_{n−1}w_n)}{C(w_{n−1})}$, where\n",
    "$C$ tells the number of occurrences in the training\n",
    "set.\n",
    "\n",
    "The probability of the entire word sequence can be computed using **the chain rule of probability**. And considering the bigram assumption, it is:\n",
    "\n",
    "$P(w_1^n) ≈ \\prod_{k=1}^{n}P(w_k|w_1^{k−1}) ≈ \\prod_{k=1}^{n}P(w_k|w_{k−1}) $\n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/n_gram_chain.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bae1d7587cae8c2bf7e3164142849876",
     "grade": false,
     "grade_id": "cell-0d9a1dfc564fd1c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Compute MLE estimates for a bigram model by hand\n",
    "## 1.1\n",
    "\n",
    "Everything is easy peasy as a concept, but there are some things to consider in practice. We need to augment our corpus by giving each sentence a special start-symbol **&lt;s>**, and a special end-symbol **&lt;/s>**. The start-symbol provides the context for the first word, so we know the words that are more likely to begin a sentence. The end-symbol makes it possible for the n-gram grammar to be a true probability distribution. Without an end-symbol, the sentence probabilities for all sentences of a given length would sum to one. \n",
    "\n",
    "Given the corpus below with all its start and end symbols,\n",
    "\n",
    "1. \"&lt;s> say hello to my little friend &lt;/s>\"\n",
    "2. \"&lt;s> say hello &lt;/s>\"\n",
    "3. \"&lt;s> say it to my hand &lt;/s>\"\n",
    "\n",
    "etimate probabilities of the following bigrams:\n",
    "* P(hello|say)\n",
    "* P(my|to)\n",
    "* P(to|hello)\n",
    "* P(say|&lt;s>)\n",
    "* P(my|say)\n",
    "\n",
    "Write your estimates as values into the variables in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bc4a77d4920943dc697862cf11bd516",
     "grade": false,
     "grade_id": "cell-d02559de98ee6bb1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p_say_hello = 2/3 # type in the answer as a number between 0 and 1. For example:\n",
    "# p_say_hello = 1.\n",
    "p_to_my = 2/3 # type in the answer as a number between 0 and 1. For example:\n",
    "# p_to_my = 1.\n",
    "p_hello_to = 1/3 # type in the answer a number between 0 and 1. For example:\n",
    "# p_hello_to = 1.\n",
    "p_start_say = 1 # type in the answer a number between 0 and 1. For example:\n",
    "# p_start_say = 1.\n",
    "p_say_my = 0 # type in the answer a number between 0 and 1. For example:\n",
    "# p_say_my = 1.\n",
    "\n",
    "#Remember to remove the raise NotImplementedError line:\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26341e484ab8129b9afaf60d0cbaba21",
     "grade": true,
     "grade_id": "cell-9f0a3220e178b273",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n",
    "from numpy.testing import assert_almost_equal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "60be2b867bde4451e33d825eac3c1e65",
     "grade": false,
     "grade_id": "cell-8f444a72729f4d30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 2\n",
    "## Building an n-gram model\n",
    "### 2.1 pad sentences with start and end symbols\n",
    "\n",
    "As we already mentioned, when dealing with language, it is very important to know what words tend to start and end sentences. To learn this with n-grams, we create special symbols of sentence beginning _\"&lt;s>\"_ and sentence end _\"&lt;/s>\"_. As a pre-processing step, you need to \"pad\" your sentences with these symbols and then create n-grams.\n",
    "\n",
    "Write a function that takes in a list of tokens, puts the needed amount of special start and end symbols in it so that we would definitely know what token or token sequence starts and ends this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68ef5e3806ed6554378feafe34658862",
     "grade": false,
     "grade_id": "cell-98a624d48aba3931",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def pad(token_list, n):\n",
    "    \"\"\"\n",
    "    this function takes in a list of tokens and pads them with special symbols\n",
    "    \n",
    "    INPUT:\n",
    "    token_list - a list of tokens to be padded\n",
    "    n - the length of a token sequence\n",
    "    OUTPUT:\n",
    "    padded_list - a padded list of tokens\n",
    "    \"\"\"\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    # YOUR CODE HERE\n",
    "    padded_list = [start]*(n-1) + token_list + [end]*(n-1)\n",
    "    \n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e9191cd890aaf0d276ddd12d6fa0870",
     "grade": true,
     "grade_id": "cell-86c59d8df560ca01",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(pad(['a','b','c'],2), ['<s>', 'a', 'b', 'c', '</s>'])\n",
    "assert_equal(pad(['a','b','c'],3), ['<s>', '<s>', 'a', 'b', 'c', '</s>', '</s>'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abbe872ba14304b0fdd0f43c2b12d776",
     "grade": false,
     "grade_id": "cell-a0f997ab8969c491",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2 convert a sentence into n-grams\n",
    "\n",
    "Let's go on and create a function that takes a list of tokens and creates an array of n-grams. The sequence of tokens is very important, so make sure your n-grams are immutable (tuples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7de4a375f215d9d2733c1186c118302c",
     "grade": false,
     "grade_id": "cell-81b21c671cf892a4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def make_n_grams(token_list, n):\n",
    "    \"\"\"\n",
    "    this function takes in a list of tokens and forms a list of n-grams (tuples)\n",
    "    \n",
    "    INPUT:\n",
    "    token_list - a list of tokens to be converted into n-grams\n",
    "    n - the length of a token sequence in an n-gram\n",
    "    OUTPUT:\n",
    "    n_grams - a list of n-gram tuples\n",
    "    \"\"\"\n",
    "    if n > len(token_list):\n",
    "        print(\"The N is too large.\")\n",
    "    # YOUR CODE HERE\n",
    "    n_grams = [tuple(token_list[i:i+n]) for i in range(len(token_list)-n+1)]\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a0d6ebd3f7cd3b3c19ac02d38660ea6",
     "grade": true,
     "grade_id": "cell-c249921375a3c149",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "\n",
    "assert_equal(make_n_grams(['a','b','c','d','e'], 2), [('a', 'b'), ('b', 'c'), ('c', 'd'), ('d', 'e')])\n",
    "assert_equal(make_n_grams(['a','b','c','d','e'], 3), [('a', 'b', 'c'), ('b', 'c', 'd'), ('c', 'd', 'e')])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04f1c44dbcc02ddd96d8ff20c2cd3794",
     "grade": false,
     "grade_id": "cell-f6809f85a87bc520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3 get n-gram counts\n",
    "Now we can collect statistics from our training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbfcf601aac567caacb11cad3ea6dbb8",
     "grade": false,
     "grade_id": "cell-83cb415fd86ec1b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_counts(sentence_list, n):\n",
    "    \"\"\"\n",
    "    this function takes in a list of tokenized and padded sentences,\n",
    "    forms a list of n-grams and gives out a dictionary \n",
    "    with counts for every seen n-gram\n",
    "    \n",
    "    INPUT:\n",
    "    sentence_list - a list of tokenized and padded sentences to be converted into n-grams\n",
    "    n - the length of a token sequence\n",
    "    OUTPUT:\n",
    "    n_gram_dict - a dictionary of n_gram history parts as keys, \n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    {('a',): {'b': 3 'c': 4}\n",
    "    \n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    n_gram_dict = {}\n",
    "    ngrams_list = [make_n_grams(sentence, n) for sentence in sentence_list]\n",
    "    for ngrams in ngrams_list:\n",
    "        for ngram in ngrams:\n",
    "            if ngram[:-1] not in n_gram_dict.keys():\n",
    "                n_gram_dict[ngram[:-1]] = {ngram[-1]: 1}\n",
    "            else:\n",
    "                if ngram[-1] not in n_gram_dict[ngram[:-1]].keys():\n",
    "                    n_gram_dict[ngram[:-1]][ngram[-1]] = 1\n",
    "                else:\n",
    "                    n_gram_dict[ngram[:-1]][ngram[-1]] += 1\n",
    "    return n_gram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04852846c239c767528ef950c3f09cf6",
     "grade": true,
     "grade_id": "cell-a2b258487651966d",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "\n",
    "assert_equal(dummy_model_3[('<s>', '<s>')], {'say': 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fe5b2bfd3128f1438980697a2c6619d6",
     "grade": false,
     "grade_id": "cell-8f605b50c75c642c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 2.4 compute an MLE language model\n",
    "\n",
    "We already have all the tools to form the items to count in a corpus, now we need to just count them.\n",
    "We've already looked at the **Maximum Likelihood Estimate** for bigrams. In general case it works like this:\n",
    "\n",
    "$P(w_n|w^{n−1}_{n−N+1}) = \\frac{C(w^{n−1}_{n−N+1}w_n)}{C(w^{n−1}_{n−N+1})}$, where $C$ tells the number of occurrences in the training corpus. Basically, you just divide the observed frequency of a particular n-gram by the observed frequency of its \"history\" part. \n",
    "\n",
    "<img src= \"../../../coursedata/notebook_illustrations/mle.png\">\n",
    "\n",
    "* 3-gram: (say hello **to**) \n",
    "* 2 words of history\n",
    "* 1 word for **prediction**\n",
    "\n",
    "Here, the whole 3-gram is (say hello to), and its history part is (say hello). If some n-gram is absent from the training corpus, its MLE estimate would be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef26f6c59a842712a2e20ce62fcb7926",
     "grade": false,
     "grade_id": "cell-d7db46c1ece8df57",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def score_mle(model_counts, n_gram, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in a dictionary of ngram counts and some n-gram,\n",
    "    and gives out an MLE estimate for this n-gram\n",
    "\n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys,\n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "        {('a',): {'b': 3 'c': 4}\n",
    "    n_gram - an ngram as tuple\n",
    "    scoring_parameters - additional, optional scoring parameters, making the function interface generic,\n",
    "        however not used here.\n",
    "\n",
    "    OUTPUT:\n",
    "    mle_score - MLE score for the n_gram\n",
    "    \"\"\"\n",
    "\n",
    "    n = len(list(model_counts.keys())[0]) + 1\n",
    "    # YOUR CODE HERE\n",
    "    if n_gram[:-1] not in model_counts.keys() or n_gram[-1] not in model_counts[n_gram[:-1]].keys():\n",
    "        return 0\n",
    "    else:\n",
    "        n_gram_score = model_counts[tuple(n_gram[:-1])][n_gram[-1]] / sum(model_counts[tuple(n_gram[:-1])].values())\n",
    "    return n_gram_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9c04730d7c936068c9a65ef17eec6e39",
     "grade": true,
     "grade_id": "cell-10fc716c6bb7dbfb",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "\n",
    "assert_almost_equal(score_mle(dummy_model_3, (\"<s>\",\"say\", \"it\")),0.33, 2)\n",
    "assert_almost_equal(score_mle(dummy_model_3, (\"say\",\"it\", \"friend\")), 0, 2)\n",
    "assert_almost_equal(score_mle(dummy_model_3, (\"say\",\"wow\", \"now\")), 0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a8bcd77a01543884e6b707a3fb65c70",
     "grade": false,
     "grade_id": "cell-eaee922237714ef5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.5  Smoothing counts\n",
    "Simple N-gram models have one very serious limitation: they are unable to give a probability estimate not only for n-grams with new out-of-vocabulary words but also for the n-grams with known vocabulary but unseen during training. The higher the $N$, the sparser the data, and the more zero counts there will be. To overcome this problem, we need to redistribute some probability mass from more frequent events and give it to the events we’ve never seen. These techniques are called **smoothing** or **discounting**. \n",
    "\n",
    "In this assignment, you will implement **Additive Smoothing**. In practice, it's not the most successful smoothing method, but it will give you an understanding of the intuition behind even more elaborate techniques.\n",
    "\n",
    "The idea of this method is that we pretend we’ve seen each n-gram $\\delta$ times more than we actually have. Typically, $0 < \\delta ≤ 1$. This way, we just add $\\delta$ to the original counts.\n",
    "\n",
    "$P(w_i|w^{i-1}_{i-n+1}) = \\frac{\\delta + C(w_{i-n+1}^i)}{\\delta|V|+C(w^{i-1}_{i−n+1})}$\n",
    "\n",
    "The $|V|$ is the (prediction) vocabulary of all possible continuations for an n-gram. It can be any word from a sentence, a special end symbol, but not a special start symbol (because it appears only as an (n-1)-gram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4af82634ea51ebd07e906b08fe217f77",
     "grade": false,
     "grade_id": "cell-d2c08bd2c805779f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def score_smoothed(model_counts, n_gram, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in a dictionary of ngram counts, some ngram and delta to be added to this ngram's \n",
    "    score, and gives out a smoothed estimate for this ngram.\n",
    "    if some word in an n-gram is unseen during training, the estimate is zero.\n",
    "\n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys,\n",
    "        where their values are a dictionary of all continuations and their counts\n",
    "        {('a',): {'b': 3 'c': 4}\n",
    "    n_gram - an ngram as a tuple\n",
    "    scoring_parameters - additional, optional scoring parameters, which make the function interface generic\n",
    "        here we will look for scoring_parameters[\"delta\"] - the delta value to be added to the counts\n",
    "                        and for scoring_parameters[\"vocab\"] -  the vocabulary that can be used as\n",
    "                        a continuation of an (n-1)-gram.\n",
    "\n",
    "    OUTPUT:\n",
    "    smoothed_score - a smoothed score for the n_gram\n",
    "    \"\"\"\n",
    "    delta = scoring_parameters[\"delta\"]\n",
    "    vocab = scoring_parameters[\"vocab\"]\n",
    "    context_length = len(next(iter(model_counts)))\n",
    "    n = context_length + 1  # the ngram length n\n",
    "    context = n_gram[:context_length]\n",
    "    word_to_predict = n_gram[-1]\n",
    "    if any(token not in vocab | {'<s>'} for token in n_gram):\n",
    "        raise ValueError(\"Distribution not defined on this n_gram:\" + repr(n_gram))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if n_gram[:-1] not in model_counts.keys():\n",
    "        denominator = delta * len(vocab)\n",
    "        nominator = delta\n",
    "    else:\n",
    "        if n_gram[-1] not in model_counts[n_gram[:-1]].keys():\n",
    "            nominator = delta\n",
    "        else:\n",
    "            nominator = delta + model_counts[tuple(n_gram[:-1])][n_gram[-1]]\n",
    "        denominator = delta * len(vocab) + sum(model_counts[tuple(n_gram[:-1])].values())\n",
    "\n",
    "    smoothed_score = nominator / denominator\n",
    "    return smoothed_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a25bbcb6178a0781738ab54a4e23a61f",
     "grade": true,
     "grade_id": "cell-aec5c000ef8b5356",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "            \n",
    "assert_almost_equal(score_smoothed(dummy_model_3, \n",
    "                                   (\"<s>\",\"say\", \"it\"), \n",
    "                                   delta=1, \n",
    "                                   vocab=dummy_model_vocab), 0.16, 2)\n",
    "\n",
    "assert_almost_equal(score_smoothed(dummy_model_3, \n",
    "                                   (\"say\",\"it\", \"friend\"), \n",
    "                                   delta=1, \n",
    "                                   vocab=dummy_model_vocab), 0.1, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a68359f7a442dc1bf15fcf963529824",
     "grade": false,
     "grade_id": "cell-14ebdff021b00d29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 3\n",
    "## How to evaluate a language model?\n",
    "\n",
    "The best way to evaluate a language model is to look at its performance in the intended application. Unfortunately, it is usually time-consuming to run the whole system just to test the language model parameters. Instead, we can measure how well an n-gram model predicts unseen data called the test set or test corpus. The higher the probability that the model assigns to the test set, the better this model performs. \n",
    "\n",
    "### 3.1 probability of a sentence\n",
    "Let's compute the probability of a test sentence. Remember we were talking about **the chain rule of probability**? This is exactly the right moment to use it.\n",
    "\n",
    "The language model probabilities are usually represented and computed in log format (as log probabilities). It has one very simple explanation: probabilities are small numbers between 0 and 1, and when we multiply these probabilities, we get even smaller product. We take a log of probabilities just to avoid numerical underflow.\n",
    "\n",
    "Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61c4e1444c7ae8fbfaa739e22ff7f878",
     "grade": false,
     "grade_id": "cell-3639f7a6c9ff750b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sentence_logprob(sentence, model_counts, score_function, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in a tokenized sentence, language model counts, and a score function.\n",
    "    it pads the sentence with special symbols,\n",
    "    and gives out its probability according to the n-gram model and the scoring method\n",
    "\n",
    "    INPUT:\n",
    "    sentence - a tokenized sentence\n",
    "    model_counts - a dictionary of n_gram history parts as keys,\n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    ('a',): {'b': 3 'c': 4}\n",
    "    score_function - a function which takes a dictionary of counts, an n-gram, and possible additional \n",
    "    parameters, and produces a score for the last token of the n-gram, given the rest as context\n",
    "    scoring_parameters - additional, optional scoring parameters, passed to score_function\n",
    "        like this: score_function(model_counts, ngram, **scoring_parameters)\n",
    "    OUTPUT:\n",
    "    logprob - a log probability score of a sentence\n",
    "    \"\"\"\n",
    "    context_length = len(next(iter(model_counts)))\n",
    "    n = context_length + 1  # the ngram length n\n",
    "    sentence_grams = make_n_grams(pad(sentence, n), n)\n",
    "    logprob = 0\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    for ngram in sentence_grams:\n",
    "        mle = score_function(model_counts, ngram, **scoring_parameters)\n",
    "        if not mle:\n",
    "            return -float('inf')\n",
    "        logprob += math.log(mle)\n",
    "    return logprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21a7c4313dd71c9d5fce5a9932c09b9",
     "grade": true,
     "grade_id": "cell-78aa138f8ce0c644",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,2) for sentence in dummy_corpus]\n",
    "dummy_model_2 = get_counts(dummy_corpus_padded, 2)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "\n",
    "dummy_test_sentence1 = [\"say\", \"hello\", \"to\", \"my\", \"hand\"]\n",
    "dummy_test_sentence2 = [\"say\", \"hello\", \"to\", \"my\", \"little\", \"hand\"]\n",
    "\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence1, dummy_model_2, score_mle), -1.791, 2)\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence2, dummy_model_2, score_mle), -float('inf'), 2)\n",
    "\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence1, dummy_model_2, score_smoothed, delta=1, vocab=dummy_model_vocab), -8.803,2)\n",
    "assert_almost_equal(sentence_logprob(dummy_test_sentence2, dummy_model_2, score_smoothed, delta=1, vocab=dummy_model_vocab), -11.106,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "836f8c8d8f394913f48be8838e4006f3",
     "grade": false,
     "grade_id": "cell-6198ad0efc830a83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In practice, the raw probability is not used as a metric for evaluating language models. \n",
    "We can look at **perplexity** and **out-of-vocabulary rate** (OOV rate) instead. \n",
    "### 3.2 perplexity by hand\n",
    "\n",
    "**Perplexity** score (PP) tells us how uncertain the model is when predicting the words in the test data $W$. You can think of it as the weighted average branching factor of a language. The branching factor of a language is the number of possible next words that can follow any word. Technically, the perplexity of a language model on some test set is the inverse probability of the test set, normalized by the number of tokens in it.\n",
    "\n",
    "$PP(W) = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1...w_{i-1})}}$\n",
    " \n",
    "In our case, let's think of $N$ as of a number of n-grams the test set contains.\n",
    "\n",
    "Let's look at the dummy language where you only have 4 letters (a, b, c, d). These letters in all possible texts appear with equal probability $P = \\frac{1}{4}$. You have a 5 letter sentence composed in this language. What is the perplexity of this dummy language as a unigram model? \n",
    "\n",
    "Write your answer as a value into the variable in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4eba266147f607808f451a8b566b129",
     "grade": false,
     "grade_id": "cell-ac36a8d340e2e90b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_ppl = 4 # type in the answer, and remove the raise NotImplementedError line.\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20145cbb99ebf199c1fd3a478be423d6",
     "grade": true,
     "grade_id": "cell-da11fb53720d74b7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### This cell contains hidden tests for the correct answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4838eca9ddf07c6df37912e2fecfd217",
     "grade": false,
     "grade_id": "cell-a038b27aa5b1d7d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3.4 perplexity as a function\n",
    "Write a function for calcualting the perplexity of a test corpus. When the model encounters an unseen n-gram, its perplexity should be infinity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78cd92404613a53e73a9ed9f594eea34",
     "grade": false,
     "grade_id": "cell-3d20eab8204b8cfc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def perplexity(text, model_counts, score_function, **scoring_parameters):\n",
    "    \"\"\"\n",
    "    this function takes in test text and the n-gram model counts and gives out the perplexity of this \n",
    "    n-gram model\n",
    "\n",
    "    INPUT:\n",
    "    text - a list of lists of tokenized sentences\n",
    "    model_counts - a dictionary of n_gram history parts as keys,\n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    {('a',): {'b': 3 'c': 4}\n",
    "    score_function - a function which takes a dictionary of counts, an n-gram, and possible additional \n",
    "    parameters, and produces a score for the last token of the n-gram, given the rest as context\n",
    "    scoring_parameters - additional, optional scoring parameters, passed to score_function\n",
    "        like this: score_function(model_counts, ngram, **scoring_parameters)\n",
    "\n",
    "    OUTPUT:\n",
    "    ppl - a perplexity score of a sentence\n",
    "    \"\"\"\n",
    "\n",
    "    context_length = len(next(iter(model_counts)))\n",
    "    n = context_length + 1  # the ngram length n\n",
    "    logprob = 0\n",
    "    num_predictions = 0  # NOTE: The number of predictions per sentence is len(sentence) + context_length\n",
    "    # YOUR CODE HERE\n",
    "    corpus_padded = [pad(sentence, n) for sentence in text]\n",
    "    n_grams = [make_n_grams(sentence, n) for sentence in corpus_padded][0]\n",
    "    prob = 1\n",
    "    num_predictions = len(n_grams)\n",
    "    for n_gram in n_grams:\n",
    "        if not score_function(model_counts, n_gram, **scoring_parameters):\n",
    "            return float('inf')\n",
    "        else:\n",
    "            prob *= 1 / score_function(model_counts, n_gram, **scoring_parameters)\n",
    "    ppl = prob ** (1 / num_predictions)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9d251cd9799a5629ba7a1001e0ed744",
     "grade": true,
     "grade_id": "cell-214c2796f178e3c2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "\n",
    "dummy_test_corpus1 = [[\"say\", \"hello\", \"to\", \"my\", \"hand\"]]\n",
    "dummy_test_corpus2 = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"hand\"]]\n",
    "\n",
    "assert_almost_equal(perplexity(dummy_test_corpus1, dummy_model_3, score_mle), 1.2917, 2)\n",
    "assert_equal(perplexity(dummy_test_corpus2, dummy_model_3, score_mle), float('inf'))\n",
    "\n",
    "assert_almost_equal(perplexity(dummy_test_corpus1, \n",
    "                               dummy_model_3, \n",
    "                               score_smoothed, \n",
    "                               delta=1,\n",
    "                               vocab=dummy_model_vocab), 4.6266, 2)\n",
    "assert_almost_equal(perplexity(dummy_test_corpus2, \n",
    "                        dummy_model_3, \n",
    "                        score_smoothed, \n",
    "                        delta=1,\n",
    "                        vocab=dummy_model_vocab), 5.4829, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b4ad1ccc93889863e79ab9c89b5727b",
     "grade": false,
     "grade_id": "cell-05d07876b4ed295e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 5 \n",
    "## Generate text\n",
    "### 5.1 visualize the model\n",
    "Another useful way to make a sanity check of how model is performing, is by generating sentences with it.\n",
    "\n",
    "1. Start with the appropriate number (n-1) of start symbols _\"&lt;s>\"_, e.g. (\"&lt;s>\",\"&lt;s>\") for trigrams.\n",
    "2. Generate tokens one at a time. Given the context, you get a probability distribution over the next word. Sample from that distribution. NOTE: For sampling, use random.choices(), calling it exactly once per token.\n",
    "3. Stop generating when the end symbol is produced.\n",
    "\n",
    "HINT: Use random.choices()\n",
    "\n",
    "NOTE: Although it should be possible to make the pseudorandom behaviour reproducible by setting a seed,\n",
    "    we were not satisfied with the robustness of that solution. Therefore, the generation functions are not\n",
    "    autograded. However, they are used later when working with real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d3560a7fc2e8e3190cef203f95f9060",
     "grade": false,
     "grade_id": "cell-2ce1b6c8fecee7fd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1.1 MLE text generation\n",
    "First we will implement text generation for the MLE case. In this case, you can simply use the true counts of each n-gram as the weights for random.choices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ddbc99d9f767bd80f35d1a6405b730fd",
     "grade": false,
     "grade_id": "cell-8abbd59bb4c0e2b6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text_mle(model_counts):\n",
    "    \"\"\"\n",
    "    this function takes in the n-gram model and produces text until the end symbol is generated.\n",
    "\n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys,\n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    ('a',): {'b': 3 'c': 4}\n",
    "    OUTPUT:\n",
    "    sentence - a sentence generated by model as a list of tokens.\n",
    "    \"\"\"\n",
    "    context_length = len(next(iter(model_counts)))\n",
    "    n = context_length + 1  # the ngram length n\n",
    "    start = tuple(['<s>'] * (n - 1))\n",
    "    end = '</s>'\n",
    "    sentence = list(start)\n",
    "\n",
    "    while sentence[-1] != end:\n",
    "        # YOUR CODE HERE\n",
    "        n_grams = []\n",
    "        n_word = list(model_counts[tuple(sentence[-2:])].keys())\n",
    "        for key in n_word:\n",
    "            n_grams.append((sentence[-2], sentence[-1], key))\n",
    "        p = [score_mle(model_counts, n_gram) for n_gram in n_grams]\n",
    "        sentence.append(*random.choices(n_word, weights=p))\n",
    "\n",
    "    # Strip the padding:\n",
    "    sentence = sentence[n - 1:-1]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04b9f830c8beb0e8f08457052181c98f",
     "grade": false,
     "grade_id": "cell-f3cbfe3afa729f52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'hello']\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "\n",
    "print(generate_text_mle(dummy_model_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42a38b26f9b52ba9c0a526a27dddd30c",
     "grade": false,
     "grade_id": "cell-842564ff5b2f8a3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 5.1.2 Additive smoothing generation\n",
    "\n",
    "Now we'll implement generating text from a smoothed model. In this case, you can use the modified counts of each n-gram as the weights. Note that in this case, you need to know the vocabulary of the model, so it is given to the function as an argument.\n",
    "\n",
    "NOTE: vocab should include the end of sentence tag!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b7fb6f70196259e640735d0fab09e1ab",
     "grade": false,
     "grade_id": "cell-5eed9b8c9273ba2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_text_smoothed(model_counts, delta, vocab):\n",
    "    \"\"\"\n",
    "    this function takes in the n-gram model and produces text until the end symbol is generated.\n",
    "\n",
    "    INPUT:\n",
    "    model_counts - a dictionary of n_gram history parts as keys,\n",
    "    where their values are a dictionary of all continuations and their counts\n",
    "    ('a',): {'b': 3 'c': 4}\n",
    "    delta - the delta value to be added to the counts\n",
    "    vocab - a set of words in the vocabulary\n",
    "    OUTPUT:\n",
    "    sentence - a sentence generated by model as a list of tokens.\n",
    "    \"\"\"\n",
    "    context_length = len(next(iter(model_counts)))\n",
    "    n = context_length + 1  # the ngram length n\n",
    "    start = tuple(['<s>'] * (context_length))\n",
    "    end = '</s>'\n",
    "    sentence = list(start)\n",
    "\n",
    "    while sentence[-1] != end:\n",
    "        curr_context_counts = {}\n",
    "        try:\n",
    "            curr_context_counts.update(model_counts[tuple(sentence[-context_length:])])\n",
    "        except KeyError:  # This context was not seen once\n",
    "            pass\n",
    "        # NEXT: Add smoothing delta, then generate the next token.\n",
    "        # YOUR CODE HERE\n",
    "        vocab_list = list(vocab)\n",
    "        if end not in vocab_list:\n",
    "            vocab_list.append(end)\n",
    "        n_grams = []\n",
    "        for word in vocab:\n",
    "            n_grams.append((sentence[-2], sentence[-1], word))\n",
    "        p = [score_smoothed(model_counts, n_gram, delta=delta, vocab=vocab) for n_gram in n_grams]\n",
    "        sentence.append(*random.choices(vocab_list, weights=p))\n",
    "\n",
    "    # Strip the padding:\n",
    "    sentence = sentence[context_length:-1]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ea3a1ec71262d283b1790817a5452cb",
     "grade": false,
     "grade_id": "cell-777b5bd912cdae3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['say', 'say', 'it', 'to', 'my', 'hand']\n",
      "['it', 'it', 'say', 'say']\n"
     ]
    }
   ],
   "source": [
    "dummy_corpus = [[\"say\", \"hello\", \"to\", \"my\", \"little\", \"friend\"],\n",
    "                [\"say\", \"hello\"],\n",
    "                [\"say\", \"it\", \"to\", \"my\", \"hand\"]]\n",
    "\n",
    "\n",
    "dummy_corpus_padded = [pad(sentence,3) for sentence in dummy_corpus]\n",
    "dummy_model_3 = get_counts(dummy_corpus_padded, 3)\n",
    "dummy_model_vocab = set(word for word in itertools.chain(*dummy_corpus)) | {'</s>'}\n",
    "\n",
    "\n",
    "print(generate_text_smoothed(dummy_model_3, \n",
    "                       delta=0.1,\n",
    "                       vocab=dummy_model_vocab))\n",
    "\n",
    "print(generate_text_smoothed(dummy_model_3,\n",
    "                       delta=0.4,\n",
    "                       vocab=dummy_model_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7231a9f7e4da53add175efb64bd6fc7",
     "grade": false,
     "grade_id": "cell-9eb98b5cfa7e7d04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## TASK 6\n",
    "## Working with real text data\n",
    "\n",
    "Let's try to model some real data. In this assignment we are working with Jane Austen's novel Pride and Prejudice, which is provided Project Gutenberg [here](http://www.gutenberg.org/ebooks/1342). In the version in the coursedata directory, the text has had some ebook disclaimers removed, as they were certainly not written by Jane Austen.\n",
    "\n",
    "First, we'll tokenize the corpus. As you remember from the Intro assignment, tokenization, processing raw text into a usable format, is not trivial. Here, we'll again use tokenizers from nltk. \n",
    "\n",
    "We'll need to split the data into sentences. The nltk's PunktSentenceTokenizer uses \"an unsupervised algorithm\" to find sentence boundaries, and a pretrained version is available for English. You may be prompted to download it. Though typically sentence boundaries are marked with hard punctuation, particularly the full stop is also found elsewhere, so the task is not at all trivial.\n",
    "\n",
    "To tokenize the sentences, we'll use the same TreebankWordTokenizer as in the Intro. This will not for example lowercase the data, which might not be ideal for all applications, but here we have no particular use case in mind, so the TreebankWordTokenizer will do. Note that this means that capitalized and lowercase versions of words are now modeled as two separate things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "60600ebda6b8627801d08f7b51137ee5",
     "grade": false,
     "grade_id": "cell-11c88e1929d9a7e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see an example sentence:\n",
      "['Bennet', ',', 'with', 'little', 'cessation', ',', 'of', 'his', 'house', 'and', 'garden', 'at', 'Hunsford', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk.tokenize\n",
    "import nltk.util\n",
    "\n",
    "with open(\"/coursedata/pride-and-prejudice.txt\") as fi:\n",
    "    janeausten_raw = fi.read()\n",
    "\n",
    "sentence_tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer() # Splits a long text into sentences\n",
    "word_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "word_detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer() # Reverses tokenization\n",
    "janeausten_sentences = list(sentence_tokenizer.sentences_from_text(janeausten_raw))\n",
    "janeausten_tokenized = list(word_tokenizer.tokenize(sentence) for sentence in janeausten_sentences)\n",
    "\n",
    "print(\"Let's see an example sentence:\")\n",
    "print(janeausten_tokenized[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bfd227f7ab5f2323505feb3504d5367",
     "grade": false,
     "grade_id": "cell-9195a2c8bf79ed63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train-test split\n",
    "\n",
    "We'll divide the data into training and test sets. The training set is used to create the model, and the test set used for evaluation. Here, we'll randomly select 10% of the sentences to be used as the test set. This way, the training data matches the test data very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84face6a17e0802512c5090059e341a8",
     "grade": false,
     "grade_id": "cell-2bc1167478df6c70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set has 5194 sentences, and the test set 577 sentences\n"
     ]
    }
   ],
   "source": [
    "test_split_index = round(0.9 * len(janeausten_tokenized))\n",
    "random.seed(808)\n",
    "janeausten_tokenized_randperm = random.sample(janeausten_tokenized, len(janeausten_tokenized))\n",
    "janeausten_train = janeausten_tokenized_randperm[:test_split_index]\n",
    "janeausten_test = janeausten_tokenized_randperm[test_split_index:]\n",
    "\n",
    "print(\"Training set has\", len(janeausten_train), \"sentences, and the test set\", len(janeausten_test), \"sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ffc0466118813b42b1780d82d511339",
     "grade": false,
     "grade_id": "cell-04de4ab989145dbb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Vocabulary size\n",
    "\n",
    "If we just use the full vocabulary of the training data, we will include some rare words, \n",
    "while excluding other words. One way to deal with this is to limit the vocabulary to some most common words. \n",
    "Everything else will be an unknown token, dealt with, together, as the \"&lt;UNK&gt;\" token.\n",
    "\n",
    "\n",
    "We will limit the vocabulary to the 1000 most common words in the training set. Additionally we include the end of sentence tag and the unknown word tag, because we want to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aecc45790876e5c491235dc8e5c6c5fa",
     "grade": false,
     "grade_id": "cell-e8f1e6a116027d92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common tokens:\n",
      " ,\n",
      ".\n",
      "to\n",
      "the\n",
      "of\n",
      "and\n",
      "her\n",
      "I\n",
      "a\n",
      "was\n",
      "\n",
      "The 990-1000 most common tokens:\n",
      " By\n",
      "application\n",
      "fancy\n",
      "congratulations\n",
      "comparison\n",
      "try\n",
      "rich\n",
      "contempt\n",
      "distance\n",
      "sooner\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "janeausten_unigram_counts = Counter(itertools.chain.from_iterable(janeausten_train)).most_common()\n",
    "\n",
    "print(\"The 10 most common tokens:\\n\", \"\\n\".join(word for word, freq in janeausten_unigram_counts[:10]))\n",
    "print()\n",
    "print(\"The 990-1000 most common tokens:\\n\", \"\\n\".join(word for word, freq in janeausten_unigram_counts[990:1000]))\n",
    "\n",
    "janeausten_vocab_1k = set(word for word, freq in janeausten_unigram_counts[:1000]) | {\"</s>\", \"<unk>\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fe7e8aa3bd1442a7c67cde952d8255c",
     "grade": false,
     "grade_id": "cell-2391bf7bd77a1703",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 6.1  replace tokens not in the vocabulary\n",
    "\n",
    "Implement the replaing of tokens which are not in the vocabulary (out-of-vocabulary words, OOVs). They are to be replaced with the unknown token \"&lt;unk&gt;\". Fill in the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fdd5ed274e0521eb168b419f74dc1295",
     "grade": false,
     "grade_id": "cell-d02c0b6e2a7711d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def replace_oovs(vocab, data, unk=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    vocab: set of tokens\n",
    "    data: list of lists, i.e. list of sentences, which are lists of tokens\n",
    "    unk: token to replace tokens which are not in the vocabulary\n",
    "    \n",
    "    This function replaces all tokens not in the vocabulary with the unknown token\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    data_oovs_replaced = list(data)\n",
    "    for i, sentence in enumerate(data_oovs_replaced):\n",
    "        for j, token in enumerate(sentence):\n",
    "            if token not in vocab:\n",
    "                data_oovs_replaced[i][j] = unk\n",
    "\n",
    "    return data_oovs_replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2085c657f08ec6a36928e8f2358e9fa0",
     "grade": true,
     "grade_id": "cell-4daf2001dce04789",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(replace_oovs({\"a\",\"b\",\"c\"}, [[\"a\", \"b\"],[\"a\",\"b\",\"c\",\"d\"]]), [['a', 'b'], ['a', 'b', 'c', '<unk>']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f86cc9a1954c8c57db6b8672ef907491",
     "grade": false,
     "grade_id": "cell-90bdfe5f57bd9bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we'll apply the filter to the Jane Austen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2f1904425dd068dffb19cdd1bdfe2830",
     "grade": false,
     "grade_id": "cell-9af163c379d54e33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see an example:\n",
      "She was received, however, very <unk> by them; and in their brother ’ s manners there was something better than politeness; there was good humour and kindness.\n"
     ]
    }
   ],
   "source": [
    "janeausten_train_1k = replace_oovs(janeausten_vocab_1k, janeausten_train)\n",
    "janeausten_test_1k = replace_oovs(janeausten_vocab_1k, janeausten_test)\n",
    "\n",
    "print(\"Let's see an example:\")\n",
    "print(word_detokenizer.detokenize(janeausten_train_1k[106]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7abd58a17c9106e64102588393695791",
     "grade": false,
     "grade_id": "cell-e59be86cd2f39931",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### MLE model\n",
    "\n",
    "First we'll train a maximum likelihood estimate trigram model. Now that we're using a larger dataset, the estimation might take some time (a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3802a0311ae158e664f3588c2697d98",
     "grade": false,
     "grade_id": "cell-25b108227c537bec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# let's pad the training corpus \n",
    "janeausten_train_1k_padded = [pad(sent, n=3) for sent in janeausten_train_1k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d2e6f73d7b0cba8ceef79b5082c5bc3",
     "grade": false,
     "grade_id": "cell-d42515202a1de004",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# now let's get the n-gram counts\n",
    "janeausten_train_1k_counts = get_counts(janeausten_train_1k_padded, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f439cf84a33c7658adf607d4900a7c60",
     "grade": false,
     "grade_id": "cell-9ea7418301cb5887",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "However, even though any unseen token is accounted for by the unknown token, we still get infinite perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21dc66704f249a8abc0d2b98caf4b54",
     "grade": false,
     "grade_id": "cell-f79809498aa30977",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(janeausten_test_1k, janeausten_train_1k_counts, score_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea6fe048ac0c418eece72f22b7d0735f",
     "grade": false,
     "grade_id": "cell-c35757b74baa6caa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Smoothed model\n",
    "\n",
    "Now, we'll smooth our counts with a few different delta values and look at perplexities. This will also take some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28339abaf47d378653d0d2a9f6dd66ab",
     "grade": false,
     "grade_id": "cell-6ce3158bd7c64819",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we can get some type of proper perplexity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284.09760341164673\n",
      "62.943456533810746\n"
     ]
    }
   ],
   "source": [
    "print(perplexity(janeausten_test_1k, janeausten_train_1k_counts, score_smoothed, delta=1, vocab=janeausten_vocab_1k))\n",
    "print(perplexity(janeausten_test_1k, janeausten_train_1k_counts, score_smoothed, delta=0.01, vocab=janeausten_vocab_1k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7a400f0b9a5894dc5e40d5464d1d8a9",
     "grade": false,
     "grade_id": "cell-1c8a5cc764ce91bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TASK: Generate sentences, comment on differences\n",
    "\n",
    "Let's see what kinds of sentences do the two types of models generate.\n",
    "Generate some sentences, and comment on the differences.\n",
    "\n",
    "- Why do the smoothed models produce such long walls of text?\n",
    "    - If we backed off to lower order models, would that help?\n",
    "- Which model generates text, that looks most like the original? Why?\n",
    "- Which model do you think would be the best language model for e.g. optical character recognition of Jane Austen's hand written notes, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dee88d0b2988bcd2e8eb0b4c4089c51",
     "grade": false,
     "grade_id": "cell-37cc13550d54f074",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences from the MLE model:\n",
      "“ Oh!\n",
      "The reason why all this <unk>.\n",
      "But <unk> ’ <unk>.\n",
      "“ You <unk>, and <unk> of Lydia on this subject, as to me about him for us; was <unk> settled, that he was again <unk> in the <unk> feelings were yet more, till the following hour.\n",
      "I, ” replied Elizabeth with some <unk>; but there were <unk> to his real character, and <unk> the <unk> of <unk> into the <unk>; that had not herself <unk> to be expected.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences from the MLE model:\")\n",
    "for i in range(5):\n",
    "    print(word_detokenizer.detokenize(generate_text_mle(janeausten_train_1k_counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca1a4c2457d8eeac55593cf1629a63af",
     "grade": false,
     "grade_id": "cell-90e43a931cdac577",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the low delta smoothed model:\n",
      "The two suffer what journey Mrs acquainted play madam once happiness ask live disposed For dancing wishes housekeeper convinced knew astonishment concerned terms comprehend so convinced me appear cousin instantly praise Their Chapter sometimes praise After how) So stay contempt nothing leave removed doing above made perhaps Forster easily avoid indifference Chapter heart conversation They success greatly should express dearest subject consent miles unable thus ago already eyes has character kindness might recollected estate elegant journey form met girls silence at these doing being prevent marry neighbourhood pounds convinced As From am affection son learnt sensible sat return reached called Miss find above desire help case either expression Are settled child excuse more letter reached bring little lost gave credit that glad not too high! beauty taken scheme _my_ address against change cold Forster voice attention _her_ between supposing person _her_ compliment when had cause its that Derbyshire receive regret excuse full go when air repeated admire work agreed ago application stay called lively these receive became they excuse only inquiries joined merely meaning share earnest bring expectation express know company accepted read silly not _my_ true aunt our interest join earnest probably giving society up week is surprised “ ever convinced step; mistaken second indeed pause immediately least expressed respect When dislike companion great reasonable temper anxiety get having scene received resentment excellent Is wedding much you Collins miles But kind partner girls Well good view pray lively pause fortunate thus and since friends thanks as cousin became enough) admire compliment distance tone meaning idea allow declared so me. would My brought manner civility that new resentment honour sense nobody company hear else fine back business also dislike Georgiana affectionate up saying five answered carriage no promise joined Perhaps character different aware long happened arrived match motive child admire trust work possible compliment Maria politeness directly earnest nature man most away excuse whenever never house one spoken servant influence absolutely leaving young this London rather turned: calling impossible everything lady remember Though regard marked Well tone “ having journey spent intended spoken general name late step Chapter young enough knowledge pretty enough breakfast continued summer felt quite agreeable acquaintance door sensible Collins side usual want assure desire fair promise regret Tuesday pleased that; morning advantage admiration gave began easy walking find once all away Phillips place sometimes best talk sometimes home Hurst ladyship moment Long vanity Rosings its drew Mr upon herself persuade it prevent capable return obliged last in sat Her himself circumstances expect door advantage wife say regard laugh farther Lizzy madam share as need accept talking till spirits lady voice sight our about satisfaction ever you. especially anyone Very opened hours highly three gone Such table ladies hand might Bennet knowing Colonel passed uncle were mention comparison principal consent gone neither expected Gardiner partner called fortunate opportunity health light followed generally hear expect talked aware short mother talking comprehend meaning delighted standing neither breakfast he consent resolved better half event find remained help inclination occurred ready avoid likely short years kept fortnight creature subject attempt _her_ sitting clothes it allow first something door Lucases Lydia with confess daughters With certain wonder Are saying purpose There everything young Maria father must At pleasing cold over anyone fear Mary evident object seemed going our greater visit Perhaps mother This marry Though hardly understand fancy night less an thing On principal written will expected praise to Let already everything formed sometimes If between family other form meet its could connections are eagerly did amiable see within poor fancy housekeeper sent written alone whose thus by met fortnight drawing-room neither bad satisfaction ladyship unable business You fear honour hours carriage neighbourhood angry too reasonable sent dance journey anything And call Brighton taking After chose brother acquaintance point Parsonage certainly above where indifference speak earnest sir pass assure wanted you says hope want ’ Had expression Such as open certain neighbourhood favour angry doing same trust me. rather different “ work present dearest consider estate vain matter because generally composure afraid affected change suppose Gardiner rather! notice gave hoped present write men arrival bad pass otherwise marry open step able find Saturday cause importance from rest circumstance attachment comes favourite cousin So reading contempt astonishment stay Colonel circumstance more <unk>, my uncle ’ the that information influence highly take meet by near sort feelings days pretty have knew made met assurance de nature way scarcely sit attention One face _you_ fortune delightful country composure minutes taken subject attended London suddenly scheme assurance delight prevailed quite however likewise event away room William Elizabeth there imagine Bennet taking mention we Well and form during point whenever till inquiries letter mean saw It my there least silly dance present eyes appeared used arrival point s praise instead dance bear felicity early _her_ himself walking excuse such understand So That him did shall saying became ill Oh brought appear greater acquainted opening do serious put advice thus) view round would none whatever winter charming sorry son behind quite Yes family asked should fixed circumstances sometimes nephew clothes yet unable hopes write set left dance instantly evening effect easy or related Wickham determined leave between knew instantly others gentlemen truth conversation thinking success chief turning spite resolution distance hear possible; might Is those That believed The express has family party reply pause room Brighton report such mother connection instantly house brother From hoped moment evening Your Such acknowledged glad regard . inclination stood attachment pause inclination already step favourite gentlemen stairs heart Jane men Not such persuade comfort Derbyshire certain child subject disposition instantly face not dancing herself sense Rosings greatest silly Phillips connections me. remained Catherine related farther afterwards hopes equally ‘ Colonel marriage paid Her fine three knowing So surprised yourself home door credit several all compliment already lost happened bring greatest best exactly pleasing suppose visit after women health real need silent was affected nature also chance assured ill Caroline myself _your_ end speak second myself kindness that Yes quite visit another wonder Sir wedding pleasant pleasure address resolution countenance understanding Maria affectionate wife danger seeing stairs aware conversation opened vanity lively brought father turning week take equally real must assurance received miles ready wedding hoped power They silly at equal says charming things _my_ assure everybody within has speaking high effect perfect do particularly life behind better table must On mean home _that_ must affection money himself sooner) hour hope proud friends but mind particular least more by expect sir invitation health acquainted least small heart comparison day pray afterwards sure desire gone estate pause arrived prevailed winter circumstances It imagine own smiled exceedingly necessary Hunsford good affection companion opportunity proper you. short considered from think want see ” saw months If saying four obliged reason ” Bourgh chose moment mistaken added London least wait proud Such his why Well living general pain serious too handsome scarcely admire account its home opened sure supposing attachment likewise should am Bingley sensible said these comfort great pounds years money these news Mr acquainted dear far sitting whenever equal delight Bourgh also absence Hertfordshire whatever thing acknowledged of prevent match object directly application women your Rosings directly _you_ few lady sensible ” returned natural married Yes longer long all has almost Lady afraid smile This Bingley many any design Fitzwilliam ever air character men sooner him company housekeeper understand delightful wife another few bear endeavour motive sort eldest regiment application respect found nor fancy occasion late like being endeavour elder change instead man knowledge serious surprised civil made making further greatly How hearing Is comes hope so advice least child long objection addressed favourite address partner high five charming generally door fortunate voice young anyone Meryton event nobody account there assured end father highly together persuade speak aunt slight your prevailed share My avoid admire then Well resolution settled believe My turning choose what daughter father servant thus His meant silent ’ prevailed use situation latter Chapter sensible journey I immediately To temper observed room disappointment perfect They favour carriage view admire Saturday rich far attentions the Mary minutes cold long situation for Let air walk excuse elder All away You easily though pain express already truth got different mean find once appeared way wait We has us says her longer still congratulations given following still reason share exactly congratulations A sitting book confess fear a disappointment help rest hours long son visit door make their delighted last view Had wrote bear new Yes persuade Miss company expect come expression on company felt join spoke call Her cousin Oh astonishment attempt cause Long than business suddenly reason effect true things walk common me. acquainted but de gone look assurance towards sake impossible entirely; sight opinion talked exceedingly going astonishment indeed No because beg cried unless attended exceedingly early weeks Hurst resolution fortunate uncle several rest pounds felt report knowing great application After ashamed friends Such charming trust marrying new just stairs elder part equal married Miss Such but leave observed indifferent neither place common tone former compliment favour things few entirely cause Lucas avoid directly trust came questions ’ account fancy seemed five was sight sister s forced view could praise wonder Why here Your composure looked; sense feel back seeing greater tolerably their care expressions alone tried resentment arrival believed both congratulations Georgiana young seeing beautiful words if sometimes A anyone light partner: admiration Wickham like pride you extraordinary after every anxious seem fortune these opportunity could read Let capable believed occasion Very marriage tone resolution sir share cried without paid head chief mind Mrs eagerly a say interest wished look Mr without step officers former sooner write found than attentions paid against motive find girl nephew town are superior ladyship park are forget importance considered round As extraordinary generally arrived alone sent impossible saw set think That contempt talking addressed afterwards surprised place face extremely something happy In _that_ persuaded half younger place anyone bring its four family regret matter morning idea even money No success weeks stairs work read capable letter Oh beauty hopes receive same hour gratitude persuaded The their truth sake hardly chief looked Parsonage Long greatly still danger real turning hand made far inclination wrote degree sit success obliged wished Forster Oh view knew elegant am But relations son as cried kind; months allow son directly dared else friend reason endeavour success rest allowed trust praise dearest chief reason _that_ opening pain “ Longbourn ignorant write written after feelings terms above girl intelligence say ” sent quite said contempt reached fine favour get gentleman _she_ laugh Bourgh son Oh dare congratulations avoid other sit recollected sight seeing equal certainly serious feel on play already as eagerly extraordinary far attention best name highly aware By months Hunsford looked form without And pretty heard trouble under need congratulations; and Mr.\n"
     ]
    }
   ],
   "source": [
    "print(\"A sentence from the low delta smoothed model:\")\n",
    "print(word_detokenizer.detokenize(generate_text_smoothed(janeausten_train_1k_counts, delta=0.01, vocab=janeausten_vocab_1k)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8102b602cc51129a79e1e0be2c4a0663",
     "grade": false,
     "grade_id": "cell-1be4b0fb3303dfb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the high delta smoothed model:\n",
      "<unk> Are afraid why speaking hand _your_ remained mother credit child disappointment minutes wishing Such it send opening bring reached arrived scarcely immediately How before company Oh natural neither dear greater _her_ de care avoid she length seemed duty wishing soon left expectation then (dislike intelligence chose short standing but call times library observed sort wish fortnight stay repeated Kitty case got advantage certainly such certain account almost speaking Indeed good Caroline father especially particular none speaking made resentment any arrived they dared Rosings comparison pass compliment saw recollected able told readily this part eye sent doing answered affectionate occurred greatest particularly event sisters niece inquiries than quite intended meet ignorant far deal learnt spoke pride report father reason affair repeated from then, politeness far taken sisters future satisfied because satisfied park Well against behind known William Long success can greatest Bennet terms object knowledge importance no absolutely amiable pleasing Lydia help weeks greatly effect ten sitting forward countenance everybody change In composure does last else natural sooner resolution dislike into taken might also compliment attentions whole A themselves work fair The kindness receiving husband admire lady met pounds window supposed night reason Netherfield tell Gardiner if advice lived considered Not opportunity hand party highly _me_ paid pounds expressed thought just turn favourite say therefore three necessary thus _her_ on questions acknowledged kindness which frequently also Mrs child form drew ‘ kindness some concern excellent they Is honour full depend and greatest liked greater weeks girl sisters elder same better meet head offer Lizzy fixed none knowledge might more indeed prevented Pemberley From You admiration help affair effect notice also . Why making expressions observed subject dared true own brought wished cause head expect last what bad concern earnest Their men better intelligence future object dancing news superior book taking put present thanks aunt your next to spent ask feelings pleased tried expression After coming Well why settled given likewise _that_ lived every beauty longer concern unless By can anything meant afraid pleasure daughter married forget it. addressed sake invitation men chose fortunate eldest something speak several any looking directly for civil purpose motive agreed least pass pounds being It one power also very manners coming will happy vanity love stairs suffer general occasion satisfied respect disposed settled meant whom indeed deceived particularly feeling himself Bingley turning _you_ room met brought saying comes Eliza many asked reasonable made far concern feelings wanted sensible leave Netherfield sake sooner much hopes marrying immediately two ladies better So receive accepted four ready cold meeting least If expressions having will hour way care younger spirits Bingley? As Elizabeth blame For consent mean over brought! prevailed opinion otherwise event up thoughts dear On concerned easy occasion hoped little period character s air trust truth companion town even excellent conversation It bear meaning receiving many dinner praise A that compliment circumstances clothes write for real receive an conduct _me_ surprised Sir through fortnight every contempt companion friends obliged called each name went supposing) used understanding she questions engaged Her rather Darcy necessary mind friends now place We Had Eliza indifference thinking dare tone heard large things you. use and civil well another almost greatly longer had No other whom mother Yes intended other being open What up respect ladyship learnt ask whether marked take marriage friend herself evening Bennet As supposed This table frequently temper day Gardiner seeing instead) walk conversation In ready attended where try ’ prevented formed an dinner surprise already giving early daughters Had recollected joined lively otherwise Mr terms ‘ regard something ladyship time case character herself praise you after Bingley cold each something observed that had either good pride How equal called, any us expression afraid way done is principal nobody\n"
     ]
    }
   ],
   "source": [
    "print(\"A sentence from the high delta smoothed model:\")\n",
    "print(word_detokenizer.detokenize(generate_text_smoothed(janeausten_train_1k_counts, delta=1.0, vocab=janeausten_vocab_1k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e67ad71a37aa5fbae174775f8b30a687",
     "grade": true,
     "grade_id": "cell-a48c96187f0db2fb",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE\n",
    "\n",
    "i) The smoothed models are capable of producing way linger text as they consider the whole vocabulary instead of only using the existing n-grams, which leaves more options for a follow-up word without hitting the end-token.\n",
    "\n",
    "ii) The MLE model as it is basically only capable of reproducing n-grmas which makes the text more similar\n",
    "\n",
    "iii) The smoothed model should perform best as we want to be capable of predicting unseen token/character patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
